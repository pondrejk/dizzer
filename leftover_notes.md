Carto
-----

  *This chapter examines the propeties of Big Data from the point of view of cartographic tradition, theory, and practice. We will focus on challenges that these properies pose to cartographic visualization, as well as on the knowledge cartography can offer to wider data-related practice*



## Wood and Eco
-- 10 cartographic codes -- intrasignification / extrasiginification
-- content / expression pane
-- focus on intrasignification codes (iconic, linguistic, tectonic, temporal (tense & durative) presentational) and how BD properties can be translated to those codes.
-- for some theoretical part think of signification levels -- elemental, systemic, syntehtic (!!! -- draw on this: "map sign systems talk to each other"), presentational

#More notes from @wood2010rethinking:
Intrasignification codes:

* Iconicity -- signs: plan information vs plan-free information -- "continuum between and beyond" these (to which extent is sign dependent on a real extent of phenomenon)
-- plan free is "disengaged from direct spatial correspondence"
* Linguistic -- map text -- "not also naming but also illuminating" (in sense of medeival illuminations?)
-- text has also iconic parameters (size for importance etc.)
* Tectonic -- map projections
-- does not produce material imagery, only apparent through iconic signs,
-- provides "syntactic superstructure for iconic code"
* Temporal -- 2 subcodes: tense -- temporal topology, duration -- temporal scale. Duration can be thick or thin -- my note: how about measuring it by the number of events per time unit?
* Presentational -- marginálie "perimap"

Levels of signification: (interpreted by the reader simultaneously, not in sequence):
* elemental -- not meaningfully-separable elements, functional vs. sign construct interpretations
* syntactic -- elemental signs in geographical context, structuring the space, percievable as a whole but despite the variety in signs -- e.g. road map
* synthetic -- sign systems talk to each other, each sign system can be a figure ar a ground at a same time
* presentational

Some direct quotes:

Formalist approach -- Klee, Kandinski, Dondis (A primer on visual latency), Wong (Principles of two dimensional design) and Bertin. Klee sees medial and active conditions of a same image plane (woods example lake plane and border). Woods alternative *sign-construct* -- lake consists of lake shoreline (a border between two qualities) and lake surface.

Hidden critique of Bertin? "Map signs have to be considered in terms of both expression and content. Formalist postures that regard inly signifiers but not signs don't cut it, since our conceptualizations of phenomena structure, even dictate, the manner in which we discourse about them" -- signifier vs sign?

When reading a map "we attend more to the syntax of the system than to to the semantic import of its components" -- my note: good for big data as individual components often make no sense and there is plenty of it.

"We distinguish blue highways highways from rivers nit because their signifiers are a little wider and a little less sinuous, but because they are striuctured differently sd systms, because htey are manifestly different landcapes" -- my note: what about perceiving the thematic overlay that is not rooted in the real world?

"Maps are about relationships. In other words, they are about how one landscape (...) is positioned in realation to another. The map synthetizes these diverse diverse landscapes, projecting them onto and indto one another, eith less than a subtle hints that one is correlative to anotheror that *this* is an agent or effect of *that*."

"There is nothing in the map that fails to signify."


# Philosophy of Big Data

@bollier2010promise
-------------------

-- John Seely Brown, Independent Co-Chair of Deloitte Center for the Edge, believes that we may need to devise new methods of theory formation: “One of the big problems [with Big Data] is how to determine if something is an outlier or not,” and therefore can be disregarded. “In some ways, the more data you have, the more basis you have for deciding that something is an outlier. You have more confidence in deciding what to knock out of the data set—at least, under the Bayesian and correlational-type theories of the moment.”
But this sort of theory-formation is fairly crude in light of the keen
and subtle insights that might be gleaned from Big Data, said Brown:
“Big Data suddenly changes the whole game of how you look at the
ethereal odd data sets.”  Instead of identifying outliers and “cleaning” datasets, theory formation using Big Data allows you to “craft an ontology and subject it to tests to see what its predictive value is.” “...The more data there is, the better my chances of finding the ‘generators’ for a new theory.”

-- “Financial markets are at least as complicated and important as the weather, but we don’t have the equivalent of a national weather service or a national hurricane center, for the financial markets.” John Liechty

-- "Large databases also open up all sorts of new business opportunities. **“Now-casting”** is helping companies understand the real-time dynamics of certain areas of life—from the diffusion of diseases to consumer purchases to night-life activity—which will have many long-term reverberations on markets. New types of **data-intermediaries** are also likely to arise to help people make sense of an otherwise-bewildering flood of information.  Indeed, data-intermediaries and interpreters could represent a burgeoning segment of the information technology sector in the years ahead."


siegfried2013big
-----------------
"...sometimes Big Data In means Bad Data Out. Wringing intelligent insights from Big Data poses formidable challenges for computer science, statistical inference methods and even the scientific method itself."

"Computer scientists, of course, have made the accumulation of all this big data possible by developing exceptional computing power and information storage technologies. But collecting data and storing information is not the same as understanding it. Figuring out what Big Data means isn’t the same as interpreting little data, just as understanding flocking behavior in birds doesn’t explain the squawks of a lone seagull."

"Many statistical procedures either have unknown runtimes or runtimes that render the procedure unusable on large-scale data,” writes Michael Jordan of the University of California, Berkeley. “Faced with this situation, gatherers of large-scale data are often forced to turn to ad hoc procedures that … may have poor or even disastrous statistical properties." -- <https://arxiv.org/abs/1309.7804>

"In particular, we emphasis on the viability of the sparsest solution in high-confidence set and point out that exogeneous assumptions in most statistical methods for Big Data can not be validated due to incidental endogeneity. They can lead to wrong statistical inferences and consequently wrong scientific conclusions."
"Besides that, Big Data often is acquired by combining information from many sources, at different times, using a variety of technologies or methodologies, Fan and colleagues point out. “This creates issues of heterogeneity, experimental variations, and statistical biases, and requires us to develop more adaptive and robust procedures,” they write. “To handle the challenges of Big Data, we need new statistical thinking and computational methods.”

... science cannot rely on the strictly empirical approach to answer questions about complex systems. There are too many possible factors influencing the system and too many possible responses that the system might make in any given set of circumstances. To use Big Data effectively, science might just have to learn to subordinate experiment to theory.


some notes from Massey -- For Space
-------------------------------------

3 common ways of thinking about space:
-- space as a surface we are placed on
-- turning space into time (space as slice through time)
-- sharp separation of local place and the world out there

^ those are failures of spatial immaginations

Her propositions
-- space is a prodct of interrelations, constituted through interactions
-- space is a condition for multiplicity, in space different trajectories coexist, space is predicated upon the existence of plurality
-- space is always under construction, never finished, never closed, it is a "simultaneity of stories so far"

Representation fixes (and deadens) time but also space, space is same as time unrepresentable in complete mimetic sense. Representation can be thought of as *spatialization* of the represented but can not be mistaken for the space itself (So beware of "spatial" metaphores that are used so automatically in present language).

Fabian 83 -- time and the other: "...a taxonomic space, indeed a map".

...not only might we productively conceptualise space in terms of relations but also relations can only be fully recognised by thinking fully spatially. In order for there to be relations there must of necessity be spacing.

-- space as a sphere of coexisitng multiplicity
-- ...maps (current Western-type maps) give the impression that space is a surface -- that it is the sphere of completed horizontality.
-- space -- not a discrete multiplicity of iner thins, rather heterogenity of practices and processes
-- everything in space has it's own temporality (history) (btw would be nice to be able to display it on a map, sort of a thermal image showing age instead of temperature). History had is own geographies too, so time and space are not contradictory, they are interrelated.
-- "everyting is connected to everything else" -- good reminder that our actions have wider implications, but unhelpful if it leads to a conception of already consituted holism (similar to what map shows). Space is potential links and connections yet to be made, loose ends and ongoing stories. "Loose ends and ongoing stories are real challenges to cartography".
-- Huggan 89 Decolonising the map... -- maps "exemplary structuralist activity"
-- maps are conceptual and a-temporal -- but ironically, given that these are maps, they are not spatial --structures

-- Rabasa 93 -- Inventing america..: The Atlas thus consitutes a world where all possible surprises have been precodified. ...As such tha Atlas is palimpsest (a complex palimpsest of allegories). -- about Mercator Atlas
-- on map ...We don not feel the disruptions of space, the coming upon diffence. On the road map you won't drive of the edge of your known world. In space, as I want to imagine it, you just might.


mayer2013big
-------------

NOW

-- Analogy: many images fast make a movie. "A movie is fundametally different from a frozen photograph. It's the same with big data: by changing the amount, we change the essence."

-- "Sometimes the constraints that we lice with, and persume, are the same for everything, are really only functions of the scale in which we operate." -- e.g. the main operting law of physics for us is gravity, for insects surface tension. Or nanotechonlogy -- at scale small enoug phyiscal properties change.


(principles for cognitive aids)
-----------
@thomas2005illuminating
Cognitive scientists have studied visual representations and the larger class of external aids to cognition. An external aid to cognition is an artifact that helps us reason about the world. A first step in developing principles for visual representations is to understand how they enable cognition [Card, 1999; Norman, 1993]. Some basic principles for developing effective depictions include the following (adapted from [Norman, 1993]):

* *Appropriateness Principle* – The visual representation should provide neither more nor less information than that needed for the task at hand. Additional information may be distracting and makes the task more difficult.
* *Naturalness Principle* – Experiential cognition is most effective when the properties of the visual representation most closely match the information being represented. This principle supports the idea that new visual metaphors are only useful for representing information when they match the user’s cognitive model of the information. Purely artificial visual metaphors can actually hinder understanding.
* *Matching Principle* – Representations of information are most effective when they match the task to be performed by the user. Effective visual representations should present affordances suggestive of the appropriate action. Another prominent cognitive scientist has suggested the following two basic principles [Tversky et al., 2002]:
* *Principle of Congruence* – The structure and content of a visualization should correspond to the structure and content of the desired mental representation. In other words, the visual representation should represent the important concepts in the domain of interest.
* *Principle of Apprehension* – The structure and content of a The subjects of mental representations and reasoning are the main focus of cognitive science, so the principles for depicting information must be based on research in cognitive science. The apprehension principle underlies the importance of research in perception. These meta-principles underscore that the biggest challenge in choosing a visual representation is to find the right one (not just any one) for the reasoning task at hand. – naive scientism? lecturing birds how to fly...


**weaponized design**
@diehm2018weaponised


– is facilitated by designers who are oblivious to the politics of digital infrastructure or consider their design practice output to be apolitical.

This is weaponised design: electronic systems whose designs either do not account for abusive application or whose user experiences directly empower attackers.

As platforms became more commodified – especially through mobile touch mediums – UX designers have progressively become more reliant on existing work, creating a feedback loop that promotes playfulness, obviousness and assumed trust at the expense of user safety.

A user story is “a very high-level definition of a requirement, containing just enough information so that the developers can produce a reasonable estimate of the effort to implement it". (definition from @ambler2014user)

When designing for the digital world, user stories ultimately determine what is or is not an acceptable area of human variation. The practice empowers designers and engineers to communicate via a common problem-focused language. But practicing design that views users through a politically-naive lens leaves practitioners blind to the potential weaponisation of their design. User-storied design abstracts an individual user from a person of lived experience to a collection of designer-defined generalisations. 

All intentionally-created systems have a set of things the designers consider part of the scope of what the system manages, but any nontrivial system has a broader set of impacts. Often, emergence takes the form of externalities — changes that impact people or domains beyond the designed scope of the system. @henriksenin2016frastructural 

Through inclusion, participatory design extends a design team’s focus beyond the hypothetical or ideal user, considering the interactions between users and other stakeholders over user stories.

In particular, security research and user experience design have significant practice and goal overlap and this relationship is often antagonistic. Both fields primarily focus on the systems of wide-scale interactions between users and technology, but the goals of the two fields are diametrically opposed; design is to create the best possible experience for a user, security is to create the worst possible experience for an attacker.

@van20133v on visulaisation: "Visualizing might not be the most technologically difficult part; it sure is the most challenging part. Telling a complex story in a graph is very difficult but also extremely crucial. Luckily there are more and more big data startups appearing that focus on this aspect and in the end, visualizations will make the difference. One of them is future this will be the direction to go, where **visualizations help organisations answer questions they did not know to ask."**

“Discovery consists of seeing what everybody has seen
and thinking what nobody has thought.”
—Albert von Szent-Gyorgyi (1893–1986)


@fisher2017making

Where Visualization Is Useful
Is visualization the silver bullet to help us make sense of data? Not
always. There are two questions to consider to help you decide if
your data analysis problem is a good candidate for a visualization
solution.
First, could the analysis tasks be supported with an algorithm? A
crisp task such as “I want to know the total number of users who
looked at Seattle” suggests that an algorithm, statistical test, or even
a table of numbers might be the best way to answer the question. On
the other hand, “How do users explore the map?” is much fuzzier.
Fuzzy tasks are great candidates for a visualization solution because
they require you to look at the data from different angles and per‐
spectives, and to be able to make decisions and inferences based on
your own knowledge and understanding.

The second question to consider is “Is all the necessary information
contained in the dataset?” If there is information about the problem
that is not in the dataset which requires an expert to interpret the
data that is there, then visualization is a great solution. Going back
to our fuzzy question about exploring a map, we can imagine that it
is unlikely that there will be an explicit attribute in the data that
classifies a user’s exploration style. Instead, answering this question
requires someone to interpret other aspects of the data to bring
knowledge to bear about what aspects of the data imply an explora‐
tion style. Again, visualization enables this sort of flexible and user-
centric analysis.


Paraphrased from @dennett2017bacteria

AI without comprehension extracts statistical patterns from what *already happened*. Human imagination and ability to envision realities that are not accessible to us by extrapolating from where we currently are, allows for foresighted design

Strong AI is "possible in principle" but a negligible practical possibility, it would cost too much and not give us anything we need. 

There is fairly sharp boundary between machines that enhance our "peripheral" intellectual powers (memory, calculation) and those that purport to replace the "core" powers (imagination, planning, decision making).

Design AI to be tools not collaborators.

Best to combine human as discerning, purposeful, foresighted agent to govern the process, but heavy lifting is left to inexorable pattern-finding algorithms in cascades of uncomprehending generate-and-test cycles that refine the search process.

Notes on sampling

a.) filtering & sampling: Subset of data is selected to wich we apply visualisation techniques. The key problem of sampling is selecting a subset that is representative of the whole dataset. Several types of sampling were designed to increcase the likelihood that a sample genralizes well to the whole population.

With simple random sampling each point has the same probability of being selected. Members of population are uniquely identified by consecutive positive integers and then pseudo-random number generator is used to select the subset of required size. As no other variables are considered, this technique can produce wildly unrepresentative samples from heterogeneous sets, where important structures or outliers can be omitted.

Systematic random sampling tries to avoid some limitations of the purely random approach to ensure main groups present in the population don't get omitted or oversampled in the subset. A random component is maintained by selecting randomly within the specified groups; the number of elements picked from each group doesn't have to be uniform, as groups can be weighted to preserve the relative importance of groups in the source dataset. Spatial equivalent of this approach is called geographically *stratified* sampling where we divide the area of interest into sub-areas (for example quadrat or hexagonal tessellation) and perform random selection within each cell. This is useful when we want an even coverage of the area of interest in the sample. Furthermore, for spatially uneven populations, we can employ *cluster* sampling, where we pick more samples from certain areas, for example from metropolitan areas — which is effectively weighting the areas by population. The third class of spatial sampling, *systematic* spatial sampling, that takes samples from evenly distributed locations, is more suited for sampling from continuous data rather than from discrete point clouds that mainly interest us in this chapter.

A straightforward way to evaluate the sampling design is comparing distribution and basic statistics such as mean and variance of population and sample. @chun2013spatial compared random, systematic and stratified sampling on three kinds of simulated trend surfaces: linear (gradual increase from one side to another), quadratic (highest value in the centre gradually lowering radially to the sides) and oscillating (several sinusoidal bumps and pits). According to their findings, stratified spatial sampling generally better preserves mean of the population, systematic spatial sampling performs better at preserving variance.

Spatial sampling differs from regular sampling, often breaks i.i.d. (independent and identically distributed) assumption, see @wang2012review for review of spatial sampling.

Sampling has many uses to speed up analysis, when avoiding assessing the entire population is feasible. Advances of big data infrastructure reduce the need for sampling out of necessity. Not only a data reduction technique, but often (esp. in spatial sciences) a data collecting convenience (collecting data at discrete locations and interpolating). Also resampling techniques (bootstrap, jacknife, off-by-one ... others) are useful for testing and validating spatial models, Bootstrap is better for distribution estimation, jacknife is superior for variance estimation  @chun2013spatial. See @kleiner2013general for evaluation of the bootstrap techniques from the performance standpoint.

Elaborate sampling methods require specific dimensions are chosen ahead of time, requiring prior knowledge and often costly pre-processing (@liu2013immens). Moreover, in vast datasets the reduction would have to be too significant. Sampling to ease point cloud visualisation doesn't seem as an adequate approach.

Sparklines

another tufte sparklines @tufte2006beautiful
———————-
https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR

A sparkline, as defined by Tufte, is “a small intense, simple, word-sized graphic with
typographic resolution. Sparklines ... can be everywhere a word or number can be: embedded in a sentence, table, headline, map, spreadsheet, graphic.”


# A friendly introduction to Bayes Theorem and Hidden Markov Models
<https://www.youtube.com/watch?v=kqSzLo9fenk>

example of hidden markov model
— from one variable (persons mood) we infer the other "hidden" variable (weather)
— we have to have some training dataset of previous runs of both variables from which we calculate transition probabilities (change from one state to another in the hidden var, e.g. from sunny weather to rainy) and emisshin probabilities (probability of a state in hiiden var causing state in other var — e.g. sunny weather makes me happy in 80% of cases)
— bayes theorem — calculating the probability of a state in hidden var based on — prior probability (percentage of sunny days in training set) + emission probabilties + state of known variable —- this calcualtes posterior porbability of a state of hidden var

— also we can calculate the transition in hidden var based on transition of known var — this gets more complicated in longer sets — *Viterbi algoritm* simplifies by storing just the most probable path throughout the run

# Bayesian surprise maps
@correll2017surprise
Biases in thematic maps:
- base rate bias
- sampling error bias
- renormalization bias

time in maps
---------------
## opseorafia vs kinematografia

@weibel1996kurt v preklade @vcihak2013ponorna -- "Muybridgea následovali ti umělci kterí se zajímali o pohyb, dynamiku, montáž a imitaci reálného života; Mareye ti, kterím záležalo na vidění, přerušní a konstrukci filmové reality. Kinematografie nakonec využila aparátem poskytnutou iluzi pohybu aby film postavila do služeb literatury, divadla a opery. Opseografie chce naproti tomu film rozvíjet dále jako outonomní umění a udělat z něj umění vidění (vnímání)."

## Spatial and temporal correlations

Causation-related questions for cartography:
- finding spatio-temporal co-location that would support causation hypothesis is in currently realized by comparing spatial patterns. The causal delays may hamper such comparison, one approach is extend the time range of records (e.g. comparing cumulative data within two choropleths can smooth the volatility in favor of the overall tendency). 
- Another approach is in looking for some general similarities between two sets of snapshots (spatial patterns) – if there is some similarity occurring at some interval then we have identified the delay interval. This is spatial but not temporal collocation. Problem: this assumes causal relationships across the whole area of pattern – how to search for delay in just a sub area? Also e.g. in networks the cause and the effect are spatially secluded -- in networks the temporal delay points to source location in the network
- Temporal but not spatial collocation – is map a good tool for displaying this (rather a bar chart? Yes e.g moving air masses – we infer the future state in place from the state in past elsewhere)
- What amount of apparent spatio-temporal collocation allows to rule out epiphenomena? Can map alone rule out a hidden common variable?
- How to map causal-like relationships, e.g. potential for causation to happen via variations of state across the area?
- overall, the ability of dynamic maps to find these collocations and link them to causation is to be assessed, but how? :)
*Small multiples*

-- small multiples good for longer time periods -- empirical research: boyandin2012qualitative (We observed that with animation the subjects tended to make more findings concerning geographically local events and changes between subsequent years. With small-multiples more findings concerning longer time periods were made. Besides, our results suggest that switching from one view to the other might lead to an increase in the numbers of findings of specific types made by the subjects which can be beneficial for certain tasks.) and irina2008novel (eye tracking)

-- small movement of clusters are hard to detect with s.m. --griffin2006comparison (We found that map readers answer more quickly and identify more patterns correctly when using animated maps than when using static small-multiple maps. We also found that pace and cluster coherence interact so that different paces are more effective for identifying certain types of clusters (none vs. subtle vs. strong), and that there are some gender differences in the animated condition.)

-- limited number s.m. to be effective -- dransch2010assessing

@tufte1998visual
-- on multiples: "Multiples represent and narate sequences of motions. Multiples amplify, intensify, meaning in images. Multiples directly depict comparisons, the essence of statistical thinking." 

Some show parallelism, some depict motion -- in such case "viewers must interpolate between images, closing up gaps." "Space replaces time as the sequencing dimension. ... Sequences of still images suffer the obvious (though no less important in being so) loss of the experiance of the passage of time, the loss of the rates and rythms of the actual motion," -- often aggravated by omitting any explicit time scale.
-- on confections: "...confection is an assembly of many visual events, selected (...) from various Streams of Story {quote from Rhushdie -- Haroun and the sea of stories}, than brought together and juxtaposed on the still flatland of paper. By means of multiplicity of image-events, confections illustrate an argument, show and enforce visual comparisons, combine the real and imagined and tell us yet another story.
-- Dataviz can roll-back the time variously tinker with data representations thus bypassing the need to use metaphors and verbal descriptions.


*Space time cubes and dimensionality*

@richards2004individuals

Space-time cube -- maybe a simplistic view of time, but also a rich source for interpreting the world as we normally experience it.

Dimensionality -- x-y-z-t, speaking of "absolute" space is doubtfull, althought instristically time and space may be absoulute, they are always measured in some arbitrary and subjective coordinate systems. 

Euler vs Langrange -- Eulerian approach: making observations at fixed spaital locations (time series, etc.), which are used to infer the time-space structure of the phenomenon under investigation. Lagrangian representation: a finite unit is tacked along its course -- coordinate frame is static and dynamic phenomena are measured relative to iths course as it moves through the space (buoy).

Collapse of dimensionality (e.g. transformation from 3D to 2D) can lead to loosing information, decreasing reliability.

(And even the) brain has a difficulty coping with systems whose 4-D evolution destroys much of the evidence of their prior states...

Large scale and long term require simplification. ...when high-dimensional system behaviour can be well described, it may be possible to identify consistent low-dimensional behavior patterns that can be represented more simply without direct reference to the underlying 4-D process.

Phase space, in. geography e.g. Melton -- four dimensional phase space of drainage basin properties...
Structure vs agency: The spatially distributed feedback between the form (structure) and process (agency) is therfore a broadly relevant conceptual framework for analysis of the dynamics of the systems...
Agent based modelling -- Regions are essentially aggregates - of smaller entities, of common characteristics -  which in turn are are 'constructed' by the Lagrangian behaviour of the individuals within them. Places are therefore emergent properties of large-sample individual behaviour in 4-D space-time.

On space-time cubes -- is a useful concept for imagining the relations between spatial, temporal and non-spatial dimensions (@guo2006visualization). Even though our spatial imagination is limited by 3-dimensionality of our everyday experience, data cubes allows us to assing visual clue to otherwise abstract database queries such as slicing and dicing. On the other hand intracting with the cube itself feels not user friendly, so intractive UIs typically use maps as slices of the cube -- so the spatial metaphor is always coherent timewise (true? maybe traces are an exception). 

Unused potential of s-t cubes becomes clear when considered together with searching for cyclical time patterns in spatial subsets (collumns of a cube) or when spatially correlating two phenomena with time delay (comparing slices). (TODO some picture of cubes to make it more clear)



