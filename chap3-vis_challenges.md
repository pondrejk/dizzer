# 3 Mapping spatial big data

This chapter explores possible cartographic solutions to high spatial and temporal density in spatial big data. Rendering technologies that power these solutions are described afterwards. The impact of big data on the digital map UI design is also discussed.


## 3.1 Visualisation challenges: spatial and temporal density

How to process a high number of data points for visual exploration, and why is it hard from the cartographic point of view? If we want our maps to be understood and used, then the human cognitive capabilities are the main guiding factor to adhere to. Graphic literacy varies across the population, not to mention accessibility requirements for various sensory conditions, but to even start addressing those issues, we first need to focus on the *legibility* as the base requirement common to every recipient.

Legibility in maps enables readers to separate signs from each other and to distinguish gradations of visual variables. @bertin1983semiology names three parameters that determine legibility in information graphics: *graphic density*, *angular separation* and *retinal separation*. Graphic density is a number of marks per square centimeter, angular and retinal separation describe the perceptible differentiation in angles (e.g. to compare shapes or segments in line charts) and in visual variables (like hue or size). The joke charts on Figure 15 illustrate the problem of extreme graphic density caused by high data load. Unlike angular and retinal separation that to a high degree result from the author's choice of symbolization, the graphic density is largely determined by constraints that cannot be influenced by the mapmaker. In the next section we will look closer on what these are.

![**Fig. 15** Big data scatter plot on the left and big data parallel coordinate plot on the right. A tongue-in-cheek reflection on the limits of graphics, modified after @fischer2015why.](imgs/bd-diagrams-joke.png)

Aside from graphic density that causes illegibility in map symbolization, another direct impact of big data on cartographic visualization stems from changes in data that happen through time. With accelerated update frequency two problems arise—data need to be processed in the real time in order to keep the picture up to date, plus there is a rapidly growing log of historical data that needs to be processed. Digital map interface then needs to adjust visualization to the most recent version, appropriately notify the user of important changes, and enable historical analysis and reasoning about the upcoming trends.


### 3.1.1 Design constraints

Let us consider the constrains that determine the graphic density in digital maps. The graphic density may vary along three axes (Figure 16):

![**Fig. 16** The three axes that influence the graphic density in digital maps.](imgs/img-design_constraints.png)

1. *Axis of scale* determines the land area displayed in the map view. Unlike print, digital map interfaces support dynamic change of scale (zoom in and out) and focus (panning) at a minimum. This is a great capability for exploring data and can help to mitigate some graphic fill issues, but also complicates cartographic design as the chosen symbolization should work on every scale level. This requires dynamic adjustment of symbology: for example, if point symbol size is kept constant across scales (default in many web mapping libraries) a larger point cluster soon becomes illegible due to overlaps. However, plain linear size adjustment can lead to empty-looking maps if the symbol size gets too small in smaller scales. For the majority of projects, there is a range of meaningful scales. Until recently, web mapping libraries supported only a limited number of discrete zoom levels (from 0 to 19, a limitation dictated by tiled raster base map sources), so there was a limited predictable range of zoom levels that cartographer needed to consider for a project. As we will describe further in this chapter (see Section 3.3.3), vector tiles allow for fractional levels so the zoom experience is smoother, therefore it is now more efficient to define a scale based function for symbology adjustment rather designing specifically for discrete steps.

2. *Axis of screen space* determines how the map interface reacts to varying screen sizes and aspect ratios.. Modern websites are required to be responsive, which means they should adjust the browsing experience to reflect the size and capabilities of the viewing device (desktop, tablet, or mobile nowadays, plus whatever comes next in the future). In many digital maps, full-featured performance on small screens is not pursued as the resulting experience is suboptimal. But this niche is certainly worth exploring in the field of thematic cartography, mainly because the proportion of mobile-first users is likely to rise. In a responsive digital map, the screen size and aspect ratio influences the size and shape of the map view which in turn needs to be reflected in the initial map scale (zoom level). The so-called *visual information seeking mantra* states "overview first, zoom and filter, then details-on-demand" [@shneiderman2003eyes], which translates well to digital thematic maps—we naturally expect to first see the whole extent of the mapped area. To provide the initial overview consistently across devices, the application needs to adjust the map view size and also calculate the correct initial zoom level to fit the area of interest into that view. Fractional zoom levels are a significant help in this, as the limitation of whole-number zoom levels often leads to an insufficient fit.

3. *Axis of data change* describes how digital map reacts to changes of displayed data. These changes can be far more complex than in case of the previous two axes as the number of possible data layers, configurations within them and interrelations between them is countless. The changes may be triggered by *user interaction* or in case of continuous data streams by *changes in data itself*. Users can change the visibility of data layers, modify visual variables, adjust the temporal scale, or change the aggregation level (drill-down and roll-up actions [@elmqvist2010hierarchical]). The range of supported interactions is defined by map authors, but the actual outcomes of these interactions can be quite surprising, especially when combined with dynamic data. Real time visualization then brings true unpredictability to the design process (called blindfolded cartography by @woodruff2015blindfolded). 

The fourth constraint is the axis of cartographer's ability—for example, failure to adjust symbolization to scale changes can result in illegibility even in cases when the screen space is sufficient and the data load is moderate. Choice of symbolization can greatly support angular an retinal separation and also battle graphic fill. The three aforementioned axes are in fact inseparable and combined together they determine how effective will the digital map be in different situations. Delineating the space of possibilities and then designing the application's behaviour accordingly requires lot of imagination, effort and testing.

The axis of data change is the one that is mostly affected by the big data properties. Let us consider the closely related subject of data processing pipelines.

### 3.1.2 Data processing pipelines

Earlier in chapter 2 we presented the information visualization reference model that describes the series of transformations that lead from raw data to actionable views. The actual *data transformations*, *visual mappings* and *view transformations* are unique for each project, and are likely to get re-evaluated along the course of development. Applications based on dynamic data bring an additional difficulty—the transformations need to be re-executed with inflow of new data after the application is already published and used. If there is a requirement for real-time processing, this needs to happen ideally without any manual intervention of application maintainers. 

The information visualization reference model is than no longer a description of the production process, rather it describes the data flow through the working system—the application's *data processing pipeline* if you will. From the cartographic standpoint, we find it useful to split the pipeline into *data space* and *visual space* (Figure 17). The main area of concern for cartographers lies in the visual space, in choosing appropriate visualization methods and graphic variables (visual mappings) and in designing user interaction modes (view transformations).

![**Fig. 17** Splitting the information visualization reference model into data space and visual space.](imgs/img-dataviz-models-spaces.png)

Visual mappings and view transformations have to cope with the design constraints described in the previous section and also with the cognitive predispositions that favor certain visual mappings and disqualify others. As we discussed at length in Chapter 1, big data pose a challenge for scaling the data processing infrastructure. The issues and solutions in data space are well described and understood, but the implications of big data on the design decisions in visual space are less researched. Furthermore, design space choices may have impacts on the processes in data space, so there is a large potential in treating the processing pipeline as a whole.

For example, any visual aggregations in the resulting digital map can get propagated back down the pipeline to make the earlier stages of the process more efficient. Simply put, there is no need to retrieve every data point individually if (a) we cannot render it (*screen barrier*) and (b) we cannot comprehend it (*cognitive barrier*). This can mean a significant improvement when performance of the application is of concern.

Hence the cartographic decisions in the later stages of the processing pipeline have impact on the overall performance of this pipeline. The choice of visualization and interaction methods has impact outward to the user but also backward to the previous stages of the pipeline. So far little has been done to asses cartographic methods from the outlook of data-processing scalability, though in the era of dynamic source data, cartographers need to expand their awareness in that direction.

Challenges of dynamic data lie both in the data space—where it is a question of efficient encoding, storing, transferring and decoding long series of records, and in the visual space—where we are interested in depicting the evolving spatio temporal relations and correlations. Again, the nature of the analytic features designed for the map interface have impact on the whole processing pipeline. 

Not all data processing challenges translate to cartographic challenges. Some basic data visualization methods are fairly immune to challenges of real-time data inflow (e.g. pie charts are not less readable when based on massive number of observations), but positional types of visualization suffer from graphic density. Cluttered base layer, labels or unexpected interaction results can all make the matters worse. Let us look at some ways of addressing graphic density in digital maps. 


## 3.2 Reducing graphic density in digital maps

Effective visualization should make the displayed phenomenon more comprehensible to viewers without occluding too much from the input data. Large point data sets tend to generate hardly interpretable congestions covering significant portions of the mapped area, therefore additional processing is needed to extract the density of point occurrence as well as spatial patterns formed by point attributes.

The process of controlled reduction of information complexity presented in the map is called generalization. Within cartographic processing, generalization can occur at the level of real objects, inside the data model but also within the cartographic product [@grunreich1985computer]. The model created by @mcmaster1992generalization define geometric conditions that may determine the need for  generalization—congestion, coalescence, conflict, complication inconsistency and imperceptibility. There are several methods of cartographic generalization, for example the aforementioned model recognizes simplification, smoothing, aggregation, amalgamation, merging, collapse, refinement, exaggeration, enhancement and displacement. For the purpose of this thesis, we are interested in large point data sets mostly suffer from *congestion* (too many graphic elements to be represented in a limited space on the map) that is best battled by *aggregation*. Aggregation is a generalization method that groups multiple individual objects and forms a new composite object [@egenhofer1992object]. 


### 3.2.1 Variants of spatial aggregation

The variants of spatial aggregation can be discerned by the type of the *composite shape*. We can recognize three strategies of defining composite shapes: data-driven aggregation, binning with arbitrary regular shapes and arbitrary with irregular regions defined by a polygon data set.

- Data-driven aggregation (clustering) is based on the idea that only objects that are congested need to be aggregated. This results in a map layer that consists of two types of objects—those representing clusters of points (composite objects) and single point objects. In most point cluster implementations, the composite object is visually differentiated and the count of aggregated objects is usually indicated by a number.

- Spatial binning divides the plain into regularly shaped grid so that every point can be assigned to a bin. Triangular, square or hexagonal tiling can be used. In addition to the shape, the proper bin size has to be considered.

- Aggregation defined by a different data set is essentially a transformation of point data to arbitrary polygons in order to make a choropleth map. The granularity of the aggregating data set influences the pattern perception.

Both point clustering and binning are implemented in popular client mapping libraries in some form, either natively, via plug-ins or by combination of other libraries.^[For example Leaflet implements clusters via the Leaflet.MarkerCluster plugin, hexagonal binning is available in combination with D3.js or Turf.js libraries.]

From the point of big data visualization it is important to note where the aggregation is performed—whether on the client, or via some preprocessing on the server. Both variants have their pros and cons. Aggregation performed on the client gets us a possibility to show original data along with the aggregates or perform scale dependent aggregation without reloading data from the server. Though, with large data sets these virtues quickly turn into burdens—with high point density, showing the original points may not add any value and recalculation of aggregates with every zoom change might pose a performance toll on the client.

In point clusters, the scale dependent processing allows to create just as many aggregates as it is required by the existing congestions. The most widely used point clustering algorithm called *hierarchical greedy clustering* (see Figure 18) starts with a random point and selects all the points that are within a certain specified radius—these points are marked as cluster members. Then the algorithm jumps to a different random point that is not part of the cluster, and the process is repeated until all points in the data set have been visited. This is done with the source point locations only in the highest zoom level. In other zoom levels the same process is repeated using centroids of the clusters from the previous zoom level [@agafonkin2016clustering].  

![**Fig. 18** Illustration of the marker clustering algorithm. The single configurable parameter is the radius of the circle zone. Adopted from @agafonkin2016clustering.](imgs/img-marker-cluster-alg.png)

Although the described algorithm proceeds fast (as the number points to be processed decreases exponentially with each zoom level), it leaves little room to adjusting the outcomes—the only configurable parameter is the cluster radius. There are some implementations that allow to set additional constraints, for example to make sure that clusters are not formed across some defined boundaries [@region2020aware]. 

Scale dependent binning (Figure 19) keeps the bin sizes constant relative to the map window—with zoom changes the number of data points falling into each bin changes, so does the area covered by a single bin. This alters the appearance of the spatial pattern with every zoom change. The bin size needs to be specified so that it ensures sufficient detail across the desired scale interval. If the bin size is too large, the variance in the spatial pattern is smoothed, if bins are too small several *no data* gaps can appear (Figure 19). Statisticians proposed several heuristics to select bin sizes for to aid sampling [@sturges1926choice; @scott1979optimal; @hyndman1995problem], though for visualization purposes we usually prefer the finest grid that is technically possible simply because it yields most informative and aesthetically rewarding maps.


![**Fig. 19** Hexbin aggregation using Leaflet with leaflet-d3 plugin. At too widely different scales the bin size remains constant. Input points are regrouped on client with every zoom change.](imgs/img-hex-sizes.png)

Per-scale visualization changes are problematic form the cognitive point of view both in binning and clustering. Changes in spatial pattern cause a loss of orientation between zoom levels. It is not possible to easily trace how the aggregations relate to what is displayed in higher or lower zoom levels. For example on Figure 2O it is hard to match the clusters between two adjacent zoom levels—their number, size, position and point count changed. Point clusters are especially taxing as they occlude the position of source points, their attribute values as well at the spatial extent of the cluster. Some implementations try to battle ease this by showing the spatial extent of the cluster on demand [@leaver2020leaflet] or by the ability to expand the cluster to see the values of its members [@mertel2020leaflet]. Scale dependent hexbins also suffer from this problem, though they are better at communicating densities within a single zoom level.

![**Fig. 20** Point clustering example in two adjacent zoom levels (screenshot from <https://www.reas.cz/atlas-cen>).](imgs/img-point-clusters.png)

As we have seen, there are perceptual arguments for keeping the true spatial coverage of the bin consistent across scales—so that the bin size on the screen changes with zoom. Such approach aligns well with the demands of big data processing that favor creating the aggregation on the server (most likely in spatial database) and pass it to the client in some vector form. This way the client doesn't load any data points that won't be displayed. Real-time data inflow can be processed on the server where all kinds of optimizations can take place (e.g. just updating the bins that actually changed, parallel processing, etc.). On the other hand, this approach can hit some storage size limits in vector tiles (we will describe in Section 3.3.3) as the amount of data per tile gradually increases (Figure 21).

![**Fig. 21** Maintaining the same aggregation unit area across scales can lead to hitting the recommended tile size constraints in some vector tile implementations. Note that with one step in tile hierarchy the number of contained mosaic cells ( the and associated attributes) is quadrupled. This may restrict the zoom range for using the aggregated layer in vector tile format. Choosing the right bin size then requires weighing between the desired density of the grid, required zoom range, size constraints per tile and the attribute load per grid cell.
](imgs/img-hex-zoomlevels.png)

For several reasons mentioned above, we find binning superior to point clustering for visualization of big data sets both from perceptual and technical standpoint. We also find this method more flexible and extensible from the cartographic point of view. In the following section we will look more closely at some interesting properties of hexagonal mosaics. 

### 3.2.2 Some aspects of hexagonal aggregation

In terms of big data visualization we are interested in spatial aggregation to predefined shape that is not dependent on the character of aggregated data. Unlike other methods (clustering, interpolation), the referential geometry does not change with data updates, which makes the computation performance quite predictable and scalable with higher data loads. Using arbitrary shape instead of any existing spatial unit (e.g. administrative districts) then brings flexibility in balancing the density of the grid.

![**Fig. 22** Comparison of selected mosaic shapes to circle. Hexagons are closest to the circle, which translates in more efficient data aggregation around the bin center.](imgs/bi-shapes-dist-to-edge.png)

When it comes to the shape of the grid cell, we can chose from three types of convex shapes that completely divide a plane into same parts: square, hexagon and triangle. The more similar the shape is to circle, the lower is the difference between the nearest and the farthest point on the border from the center (Figure 22). In other words, hexagon is the most compact of these shapes, which allows it to form the kind of mosaic that only has one type of neighborhood (Figure 23). Centroids in the hexagonal mosaic form a triangular grid, so an individual hexagon has the same distance from all of its neighbours. Hexagonal mosaic is therefore the most efficient and compact division of two-dimensional plane.

![**Fig. 23** Types of neighbourhood in regular shape mosaics.](imgs/binning-neighbourhoods-3.png)

The vertex type of neighbourhood can cause visual ambiguity when judging the compactness of regions in the grid—the edge neighbours of the same value in a rectangular grid may be perceived as members of the same region or as separate entities. In that matter, not having vertex neighbourhoods in the grid makes spatial patterns seem  mor contiguous. Furthermore, the straight orthogonal borders of the rectangular grid form a mesh that may attract more visual attention than the values or the cells. Hexagonal cells, on the other hand, are grouped along three axes rather than two, which yields more varied, less  rectilinear shapes. Single type of neighborhood is also convenient for modelling paths in a grid. As a disadvantage, unlike rectangles, hexagonal grids cannot form nested grids of the same shape. Grid hierarchy in hexagons is treated either by using partial hexagons or forming non-hexagonal higher level grids [@sahr2003geodesic]. For more thorough comparison of rectangular and hexagonal grids see @birch2007rectangular.

From the cartographic point of view, there are other aspects of hexagonal grids that are interesting. If the grid is used for collecting spatial samples, it should be projected to the cartographic projection of the base map so that each cell really covers an equal area. This may become a problem when the grid is used as an un-projected graphic over an inevitably distorted small-scale map (for example hexagons on Figure 19 cover gradually smaller area from south to north especially on the left image—due to the distortion of Mercator projection).

The assignment of value to the mosaic cell is another topic to consider. Predominantly, the fill color is used to denote the point count or density within the region. If we are more interested in the attribute variation, we can assign color based on some statistic of member point attributes—mean, median, variance, etc. Each of these choices come with a toll (like hiding outliers) and should be tailored to the context of the visualization or user-adjustable. Also any classification method will also have impact on the overall visualization. Apart from the classical selection (equal interval, Jenks, quantile, logarithmic...) there are also newer promising contributions to the classification problem like *head/tail breaks* [@jiang2018complex], *bayesian weighting* [@correll2017surprise] or *uncertainty-adjusted scales* [@correll2018value]. There is also a possibility to split cells to make them work similarly to pie or stacked charts [@lumley2015multi].

Multi-parametric cartographic visualization can also employ hexagonal grids, this time as an outline for proportional symbols, compound charts or other visual artifacts. The shape of the hexagon allows for various kinds of proportional splitting and versatile symbol placement strategies (some experiments are presented in Chapter 4). Notice that using hexagonal grid as a symbol placement outline disrupts the impression of a continuous surface typical for colored grids. Also it precludes applying masks to the hexagon layer to reduce the visual load and to provide geographic context (a.k.a. dasymetric method, see Figure 24)

![**Fig. 24** Dasymetric method is not applicable to most multivariate hexbin visualization methods](imgs/img-textures-and-buildings.png)

Then there are approaches that try to combine density and attribute visualization either by employing a bipolar color scale or by placing supplementary signs to the grid [@mertel2021regular]. If we are interested in the proportion of parameters within the cell, pie charts can be neatly placed to fit the hexagonal grid. This way we can also compare densities of multiple point data sets. Trying to visualize both density and attribute information of one data set leaves little room for comparison of multiple data sets, though several map views can be employed. Another approach to multi-parametric visualization is in proportionally scaling the grid cells [@weckmuller2019hyper].

There are many other interesting aspects to hexagonal grids that are not directly related to visualization but may have some applicability in digital maps, for example coordinate systems, coordinate ranges or reflections withing the grid [@patel2020hexagonal]. Hexagonal grids are also apt for use with back-end tools to build approximate queries with bounded error or response time from data sets that are too large to permit full aggregation in real time [@agarwal2013blinkdb]. Another use case for hexagonal grids is *online aggregation*—showing continuously updating aggregates and confidence intervals in response to a stream of samples [@hellerstein1997online; @fisher2012trust].


### 3.2.3 Symbology fine-tuning

Having described aggregation as a tool for addressing congestion in point layers, we must note that there is a whole range of other issues that contribute to the visual clutter in maps that cannot be easily tackled by mere aggregation. For example visual conflicts of symbology between map layers are nothing uncommon in thematic cartography and there are several strategies we can employ to the rescue. Digital environment makes some things easier and complicates the others. 

Adjusting symbology properties is possibly the simplest first solution. Conflicts in the map field are often caused more by conflicting symbology rather than location. Especially for point features the symbology covers up more space than the real spatial extent of the phenomenon, even more so if the symbol size conveys meaning—in proportional symbol maps. Classical techniques to deal with the problem include adjusting the symbol size gradation to reduce overlaps, taking care of the drawing order so that the smallest symbols appear on top the larger ones, increasing fill transparency so that the overlap situation is explicitly visible, or displacing symbols and adding leader lines pointing to the correct location (Figure 25). These methods have all their disadvantages, to name just a few: symbol size adjustments rarely work equally well across the whole map area and a break in the symbol size gradation (e.g. scaling down the largest outliers) works against intuitive inference of relationships from the map; drawing order enforcement fails if two similarly sized symbols are very close to each other; increased transparency can create distracting visual artefacts, and displacement extends the problematic cluster area plus it is a quite daunting manual task. Furthermore, maximizing the visible perimeter of all symbols is NP-hard [@cabello2010algorithmic]. Needles to say that additional theme layers further complicate the situation as new cross-layer conflicts and overlaps arise. Congested areas in one layer generally preclude seeing trough to what is happening in lower layers.

![**Fig. 25** Congestion resolution in proportional symbols—(a) reordering, (b) transparency, (c) scaling down, (d) displacement.](imgs/img-conflict-res.png)

The question of classical de-cluttering techniques becomes more interesting if we turn our attention towards automating them. The automation will increasingly become necessary because with dynamic systems we do not know beforehand how the data will look like—the congestions will arise dynamically and will have to be resolved on the fly. It certainly helps if some data properties can be estimated beforehand (e.g. the atmospheric temperature in some location is unlikely to break out of its previously measured bounds by an order of magnitude). At first glance, the majority of the mentioned techniques seems to be easily achievable within current digital mapping libraries.^[Maybe with the exception of symbol displacement, chart rendering libraries like D3.js support collision resolving in bubble charts, though it is not being widely used in the context of digital maps] But if we put the current implementations under close scrutiny, even playing with something as simple as symbol transparency can yield some unexpected variants that highlight the limitations of how we currently render maps on the web (see Figure 26).

![**Fig. 26** A simple exploration of handling proportional symbol overlap. (a) The rendering order of symbols is from the largest to smallest. This can prevent hidden symbols, but doesn't allow to see the underlying layers. (b) Transparency alone can permit a glimpse trough, but the overlapping parts in the symbology create a distractive visual artefact, also it can be suboptimal for heavily congested areas (small symbols in the centre are now illegible). (c) Darker outlines ensure all symbols are visible, but also contribute to the visual clutter in the overlapping parts. (d) One might attempt to omit fills altogether but that tends to make the irritation of overlaps even bigger—the artefacts created by intersections now visually compete with the actual symbols. (e) This is not the variant of *(a)* with white fills, here the fills are blank but overlapping borders are removed, which means the fills act as opaque when clashing with symbols in the same layer (removing the problem in *(d)*) but are fully transparent towards lower layers. From now on we move beyond what is readily available in client mapping libraries. (f) Combination of *(b)* and *(e)* can be useful when having fills is important e.g. for added symbolization. Toned down outlines help to identify individual symbols. We can say there are two modes of transparency within one symbol layer: while outlines are self-opaque (not visible in the self-overlapping areas of the symbol layer) the fills are self-transparent (overlaps are shown). Both fills and outlines are transparent towards the lower layers. (g) Variation of *(f)*—some displacement between the fill and the slightly transparent outline creates an interesting 3D effect. Though this is mainly a toy effect as it is highly dependent on the clean background—for small symbols in the center it is not visible. (h) An example of how *(e)* might work in combination with another symbol layer. (i) A reversal of *(e)* where fills are solid towards lower layers but outlines are fully transparent. The progression of the border in the underlying layer is observable through transparent outlines in the symbol layer. (j)(k)(l) Additional complexity arises from using color scale to visualize an additional data attribute. Both fill and outline can be used to carry color. In theory, there are several possibilities to ensure legibility even for two overlaying bi-parametric symbol layers. The assignment of fill and the outline width determine which layer will seem dominant. In interactive environments user could change symbolization to visually reorder layers while maintaining the overall picture.](imgs/img-circle-overlays-2.png)

The images in Figure 26 were created using a desktop vector graphic programme (Adobe Illustrator) by manually playing with shapes not linked to any data or coordinate system. Adobe Illustrator features like *flatten transparency* were used to combine multiple layers to create the impression of dual transparency and other effects in pictures (f)—(l). As such, the process is not translatable to digital maps based on dynamic data. To our knowing, graphic effects (e)—(l) would not be achievable neither in current web nor desktop mapping platforms.^[Though in pure SVG it is possible to use the mask element to achieve similar effects, see <https://stackoverflow.com/questions/66000769/is-there-a-way-to-draw-shapes-so-that-their-fills-act-as-opaque-within-the-group>]

It seems that current cartographers have to split their energy between three types of tools that are each good for some tasks but fairly deficient in other areas. Desktop graphic software like Adobe Illustrator or Inkscape allows for graphic idea profiling and advanced vector manipulation, desktop GIS solutions like QGIS or ArcGIS provide a whole arsenal of spatial data manipulation tools, and web mapping solutions like Leaflet are great for interactive data presentation. These tools are not very interoperable and the synthesis is nowhere near.

Often the visually problematic areas in the thematic layer do not span through the entire map field. Rather there are some clusters of symbols with high self-overlap and the rest of the field is distinguishable without treatment. Static maps deal with such clusters by insetting another map field in larger scale focused on the problematic area (common for example in socio-economic maps of Poland where the densely inhabited region of Silesia almost always exhibits symbol clutter). Although technically nothing prevents adding insets to digital maps, it is not seen very often in practice. Maters would be worse in real time visualization of dynamic data as the position of problematic areas is not known beforehand. Here the automatic cluster detection would make sense not only for creating inset maps as needed but also for tracking the spatial and temporal distribution of clusters.

In digital maps we are not confined by a fixed scale, therefore we can zoom in at the areas of interest. In this interactive environment, leaving congested areas without treatment might not be such a big problem. Even though the clusters are congested at the general resolution scale—the congestions suggest that there is something interesting going on in the area and invite users to place their focus there. However, as we zoom in we loose sight of the overall pattern—which can be solved by a different kind of inset map, this time for statically displaying the full area of interest as well as dynamically marking the sub-area that is currently displayed in the main view. As we will describe in Chapter 4, modern front-end development frameworks support creating modularized reusable components that can share data, which is excellent for implementing multiple co-ordinated map views.

Zooming and panning are interaction modes that for many users basically define what interactive map is. But it is peculiar that beyond these modes we don't see much more dynamic interaction options in digital maps. WebGL-based client libraries added the change of orientation and tilt around the z axis but when it comes to interacting with the thematic layer, we are usually left with some selection, filtering and on-hover pop-up bubbles that act as on-demand labels for selected features. Rarely the user is allowed to tamper with the actual symbolization. The linked view interfaces like GeoDa or more recently dashboards like Grafana rely on amassing several visualization types to display various aspects of the same data set, but the actual cartography in these interfaces is very simple (usually a choropleth). But for interacting with massive dynamically rendered data sets there must be more options provided for users to moderate the cognitive load, including change of symbolization, visual weight and order of thematic layers.


## 3.3 Rendering Spatial Data  

How is the cartographic design influenced by the rendering technology employed? What technology (or combination of technologies) is suitable for cartographic visualization of dynamic data sets on the web? In this section we will describe the tree main technologies the current web development toolbox offers for showing interactive graphic information. We will mostly focus on WebGL with brief description of how it governs the GPU rendering pipeline. Then we will describe how are these technologies baked into web mapping libraries. We will also describe the vector tiles specification.

### 3.3.1 SVG, Canvas, WebGL

**SVG** is a well known and much loved format for displaying two-dimensional vector graphics on the web. Since the start of development in 1999 it has become an often used alternative to bitmaps with wide browser support. Unlike the remaining two technologies, SVG is a vector format, which implies scalability^[The acronym stands for Scalable Vector Graphics after all], constant graphic quality across devices, and smaller storage size. The straightforward XML syntax allows for easy integration with JavaScript and CSS. SVG files can be easily autogenerated, searched, compressed or indexed by web crawlers. SVG is an example of the *retained mode* graphics model, were graphic library constructs a scene from primitives defined by a declarative API and keeps the model of the scene in memory. Elements of an SVG graphic exist in the site's DOM which allows for attaching JavaScript event handlers to sub components of a graphic. This feature makes SVG a good choice for implementing interactive graphics—it powers popular web charting libraries like D3.js. Also, it is a default technology for data overlays in web mapping library Leaflet.

On the flip side, SVG graphics being a part of the DOM quickly becomes a bottleneck when working with large dynamic data sets or when animation is required. @eberhardt2020rendering reports that SVG charts can typically handle around 1000 datapoints, @baur2015weighing mentions a maximum limit of 10000 static elements and 1000 animated elements.^[For a simple benchmarking site to do your own experiments (with caution) visit <http://dominikus.github.io/stars/>]

There are several ways one can animate SVG graphics: CSS, a handful of JavaScript libraries or the native SMIL animation specification (see @drasner2015weighing for performance comparison). However, SVG performance decreases quickly with growing number of items to display, which disqualifies it for visualizing live data streams where we don't know the load beforehand.

**The Canvas API** provides a means for drawing graphics via JavaScript and the HTML5 *canvas* element. Among other things, it can be used for animation, game graphics, data visualization, photo manipulation, and real-time video processing. The canvas element exposes a bitmap surface where you can programatically draw rasterized images. It is an example of the *immediate mode* graphics model, were the scene is rendered directly based on procedural API and no object representation of graphics is stored in memory. This makes it harder to debug (as for the browser the canvas is just an image), but for visualising dynamic data streams it shields us from the risk of bloating the DOM. Canvas supports only resolution-dependent (raster) visualization which might be an issue when rendering textual labels.

Compared to SVG, Canvas allows for more low-level control of the rendering process, the API is powerful but may be demanding on developer when it comes to defining objects and user interaction with rendered elements. Canvas is an engine for drawing pixels, natively there are no objects to attach event handlers to. For such capabilities it is possible to choose form a broad range of wrapper libraries.^[See the list at <https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API#libraries>] The animation performance for large number of objects is definitely a virtue compared to SVG—you can expect to render around 10000 points whilst maintaining smooth 60fps interactions [@eberhardt2020rendering]. In terms of mapping libraries, Canvas is a supported alternative option for rendering data overlays for example in Leaflet. 

**WebGL** is a JavaScript API for rendering high-performance interactive 3D and 2D graphics within any compatible web browser without the use of plug-ins. WebGL is younger than SVG and Canvas, though its browser support has already grown broad.

WebGL API is based on OpenGL ES. Originally developed in the late 1980s, OpenGL has been an industry-standard API for programming 3D graphics. OpenGL ES (for “embedded systems”) is the version of OpenGL developed to run on small devices such as set-top TVs and smartphones [@parisi2012webgl]. OpenGL provides a special C-like language—GLSL (OpenGL Shading Language) to write programs that are directly executed by the GPU (Graphics Processing Unit).

Like the Canvas API described above, WebGL uses the HTML5 canvas element as a rendering engine, and also belongs to the immediate mode style of graphic APIs. Unlike SVG and Canvas, WebGL employs hardware acceleration on the client machine, which allows for high rendering speed. It has been mainly used for powering online gaming, but it is also more than suitable for real time visualization of large data sets.

On the other hand, the complexity of the developer experience is seen as the main obstacle (drawing a simple coloured triangle in plane GLSL takes around 40 lines of code). The WebGL JavaScript API does not provide any form of abstraction over the underlying GLSL language [@eberhardt2020rendering]. However, there are wrapper JavaScript libraries that provide some object oriented features over WebGL. But from the cartographic standpoint the greatest improvements came with the onset of the vector tile format and related WebGL-based mapping libraries. But before diving into these advances, we will briefly describe how the GPU renders graphics, and how GLSL allows us to control that process.   

### 3.3.2 GLSL and the GPU rendering pipeline

As we have outlined above, WebGL provides a JavaScript API that allows to create and manipulate GLSL constructs (called shaders) that access pixels and vertices the GPU works with (Figure 27). In the simplest terms, the GPU decides how to use the pixels on the screen to create an image. GPU is a piece of hardware designed specifically for performing the complex mathematical and geometric calculations that are necessary for graphics rendering. These calculations are done in a massively parallel and hardware accelerated manner (math operations are resolved directly by microchips rather than by software), which makes the computations many orders of magnitude faster compared to the same computations performed on the CPU. The GPU understands vertices, textures, and little else; it has no concept of material, light, or transformation. The translation between those high-level inputs and what the GPU puts on the screen is done by the shader [@parisi2012webgl].

![**Fig. 27** Illustration of the basic WebGL concepts. Triangle is a base graphic primitive from which the more complex graphics are built. The canvas provides a drawing context where the viewport is initialized. The viewport delimits the operating space and a coordinate system. Vertices A, B, C hold information on coordinates and color (besides other data), the triangle fill is interpolated from vertices. The close-up circle shows the fragments (pixels) that make up the overall image. Both vertices and fragments can be directly manipulated by GLSL.](imgs/img-webgl-elements2.png)

Shaders are pieces of GLSL code that define how vertices, transformations, materials, lights, and the camera interact with one another to create an image. There are two kinds of shaders:

* The *vertex shader* provides the code for converting object coordinates to the 2D rendering space. It will run once for every coordinate that is passed to WebGL. It can be used to transform, scale, or otherwise mutate a shape.

* The *fragment shader* provides the code for determining the color of each drawn pixel. It is run separately for each pixel (possibly in multiple passes) to generate the final color value based on input data such as color, texture, lighting, and material values.

To understand shaders a bit clearer, let us describe what an application needs to do to render WebGL graphics as well as what are the steps the GPU performs to make it possible. The factual base for the following section is owed to @parisi2012webgl, @agafonkin2017how, @vivo2015book, @ademovic20163d, and @oconnor2017gpu.

In order to render WebGL onto a page, an application must first obtain a drawing context for the canvas and then initialize the viewport in it. To WebGL, viewport is conceptually a 3D space, spanning between -1 and 1 on the x, y, and z axis (z axis is used even for 2D graphics to perform depth checks). Within this space, drawing is done with use of primitives, which are geometric objects (mostly triangles but also points and lines). Primitives use arrays of data, called buffers, which define the positions of the vertices. The application creates one or more vertex buffers and passes them to the GPU RAM. Next, the (simplified) graphic pipeline goes as follows (see also Figure 28):

![**Fig. 28** A simplified diagram of the GPU rendering pipeline, description in the text.](imgs/img-graphic-pipeline.png)

* *Input Assembly.* The GPU reads the vertex buffers from memory (the general purpose RAM) and generates the vertex buffer objects (VBOs) within the GPU memory. VBOs in general store an array of coordinates for vertex positions (sometimes also normal positions^[Normal refers to a vector perpendicular to the surface that informs about the orientation of the surface facet. Storing normal vectors for vertices allows to calculate the lightning of the scene.]). GPU determines how the vertices are connected to form triangles, and feeds the rest of the pipeline.

* *Vertex Shading.* The vertex shader gets executed once for every vertex. Its main purpose is to transform the vertex coordinates from the object to the viewport coordinates. This is done by applying matrix operation on each vertex taking into account the current camera and viewport settings to calculate where it will end up on the screen. There are three types of variables that go in and out of a vertex shader: *attribute*—variables that hold specific properties of a vertex like position or normal. These variables are subject to matrix transformations; *uniform*—variables that are the same for every vertex within the same rendering call (values that describe the whole object, pointers to textures); *varying*—variables that will later get passed to fragment shaders, like color or transparency. These variables describe describe the surface between vertices.^[There are many optional operations like tessellation or clipping that can be done to vertices before rasterization, we omit them here for brevity.].

* *Rasterization*. Once the vertex shader has run on each vertex of a triangle and the GPU knows where it will appear on the screen, the triangle is rasterized – converted into a collection of individual fragments.^[Fragment is a term used for pixel in the making as not all fragments get actually rendered as pixels on the screen. But the terms fragment and pixel are often used interchangeably] Per-vertex values (coordinates, vertex color, normal, etc.) are interpolated across the triangle’s fragments. After reasterization and interpolation an early depth test is performed, which is basically a comparison of z coordinates of fragments form overlapping objects. Fragments that would eventually be occluded are removed to save time.

* *Fragment shading.* Each rasterized fragment is then run through the fragment shader. This gives the fragment a color by combining material properties, textures, lights, depth and other parameters in the programmed way to get a particular look. Since there are many fragments and each one needs to be shaded at least once, the fragment shader is usually where the GPU spends a lot of its time. Input values from the rasterization step are processed to a single output value per each fragment—an RGBA array that defines the fragment color and opacity.

* *Rendering target output*. Finally the fragment is written to the render target – which is usually a frame buffer (a memory object storing values of pixels in the canvas). Before that, several validation tests are performed, depth check discards occluded fragments and transparent fragments are blended.

The fact that shaders are executed on the GPU means that developers cannot rely on console output for testing and debugging as they are used to with programs run on the CPU. Instead they need to find a way to express their test or debugging condition in terms of color that is the only output of shader programs. Moreover, shaders are run for every vertex or fragment in parallel. Running in parallel means that the execution thread is "blind" to what other threads are doing. There is no way to check the results of execution in one thread form the other parallel thread or pass data between threads. Also there is no persistent memory that would store the previous computation results for fragments, so with changes the whole scene is rendered anew. These properties make shaders not very popular among beginning programmers [@vivo2015book].

To improve the developer experience, a number of wrapper libraries have been created to provide high-level, developer-friendly features on top of raw WebGL.^[Implementing such wrapper toolkits was made possible thanks to the performance increase in web browsers’ JavaScript virtual machines] Libraries like *Three.js* or *Pixi.js*^[Even though these libraries are primarily intended for game development, they are not without potential for cartographic visualization. Three.js provides a toolbox for rendering 3D scenes, Pixi.js is focused on creating 2D games by rendering pre-created raster images (sprites) but it has also been successfully used for cartographic visualization in @escoffier2017how] make WebGL programming more accessible as a lot can be done in them using pure JavaScript. There is also a growing number of libraries supporting geospatial features we will describe in the next section.

It remains to add that the GPU rendering pipeline can also accept bitmaps as an input that is used mostly to generate textures during the vertex shading phase. Bitmap swatches are applied repeatedly in a mosaic to create texture or to combine with values calculated from vertices (see Figure 29). We can conclude this section with a note that GPU-aided computation is likely to become more commonplace even beyond the field of computer graphics—GPUs are already employed to preform heavy matrix calculations in libraries like *Cuda* or *TensorFlow*.


### 3.3.3 Tour of Vector Tiles

How is WebGL relevant for cartographic visualization? To answer this question, let us begin with the first stage in the GPU rendering pipeline and a sub-question: how to efficiently load spatial data into the GPU? One of the options is a recent data-transfer format known as *vector tiles*. Vector tiles build on some concepts inherent to raster tiles that were until recently the most popular form of serving maps online. In many aspects the two formats are far apart, so let us first briefly describe raster tiles for comparison.

*Raster tiles*

The idea of raster tiles is basically to divide the world map into manageable sections (tiles) that can be easily served over the network. In the browser, these tiles are assembled to create a seamless impression of the map for the specified area.

Tiles are square images (usually PNGs in 256×256 pixel dimensions), each with a fixed geographic area and scale. Each tile is assigned a quadratic key that determines its position in the scale hierarchy, tile coordinates and the zoom level.^[Tile coordinates are based on a hierarchical square net over the Mercator projection. As such, coordinates differ per zoom level: x goes from 0 (left edge 180 °W) to 2<sup>zoom</sup> − 1 (right edge is 180 °E); y goes from 0 (top edge is 85.0511 °N) to 2<sup>zoom</sup> − 1 (bottom edge is 85.0511 °S). The upper and lower bounds are cut off so that the whole world is encoded as one square.] Tiles are stored in a hierarchical file structure or a database, so that the server can select and serve the needed tiles upon request.

To consume raster tiles, the client mapping library (say Leaflet.js), based on initial parameters (viewport size, initial coordinates of the central point and the zoom level) requests the tiles from the server. User interactions like pan and zoom trigger requests for additional tiles. The tiling keeps the amount of transferred data at bounded and predictable levels. On the flip side, the tiles (being plain raster images) have no concept of what data are displayed—feature rich tiles have the same storage size as the "empty" tiles (e.g. single-color ocean areas). Furthermore, tiles for the feature-empty areas need to be generated for all zoom levels the map supports. 

There are several tools to generate raster tiles form any spatial data (*Mapnik* is the most used open source engine to do that). Styling the data and rendering the tiles is the first step in the process, which is not very flexible when there is a need to recreate tiles based on changing data. Client is merely a consumer of the tiles that are not easily adjustable to client-specific needs—for example translation of labels based on browser locale is not possible unless there is a parallel language-specific version pre-rendered on the server.

Raster tiles have several specifics also from the cartographic point of view. Fixed tile size dictates discrete zoom levels. The cartographic decisions regarding the map appearance are done at the side of the tile provider. For client-based thematic mapping, the tiles are mainly used as a base map that is beyond cartographer's control.^[If we exclude tinkering with CSS and Canvas filters like in <http://humangeo.github.io/leaflet-tilefilter/demo.html>] Client libraries natively allow for SVG and Canvas overlays for custom data, the mapmaker's task is to select an appropriate base map from the wide selection of providers.^[For continuously updated overview see <https://leaflet-extras.github.io/leaflet-providers/preview/>] Clashes between the thematic layer and the base are treated by adjusting the thematic layer or by choosing a different base map.

We will now focus on the advantages of vector tiles, but that is not to say that raster tiles are no longer used or useful. Even though there are no utilization statistics available, with large map providers like Google Maps moving to vector tiles (in larger scales) we can assume that vector tiles are in majority when it comes to page views. That being said, raster tile sources are still demanded and used in many thematic map projects, not to mention legacy implementations. Also, some data sources warrant raster encoding, e.g. aerial photography or scans of heritage maps.

*Vector tiles*

As the name suggests, vector tiles use vector data instead of raster images. 
Vector tiles build on a data exchange format called *protocol buffer* (or protobuff). Protobuff is a binary format that enforces *schema* on encoded data, which makes it much more efficient in storing structured data than human-readable formats like JSON, XML or CSV.

The hierarchical structure, the addressing system, spatial extents and zoom levels of common vector tile implementations are the same as for raster tiles described above. The geometries are encoded within the extent of individual tiles, the tile itself contains no information about the geographic bounds and projection [@mapbox2020vector]. The individual tiles are then stored as protobuffs in a hierarchical folder structure for serving based on zoom levels.^[Mapbox implementation supports a wrapper format .mbtiles that is essentially an sqlite database file.]

Unlike raster tiles, vector tile also encodes feature attributes. Internally, the tile is structured so that it contains one or more layers comprised of features with defined geometry type, geometry and attributes [@mapbox2019vector]. The protobuff format supports concatenation, which means we can easily add or combine layers from various sources. 

In contrast to raster tiles, rendering happens at the very end of the data exchange process between server and client. The client library is responsible for requesting the tiles, applying the style information and passing the instructions to the rendering pipeline on the client (geometry and styling information from tiles is fed to GPU in form of vertex buffers—see Figure 28, one rendering per tile per layer).

There are four general types of software that used to manage various aspects of the vector tile life cycle:^[There are numbers of concurrent implementations for each of theses groups, for an updated overview see <https://github.com/mapbox/awesome-vector-tiles>] 

* *Parsers and generators*: libraries that read and encode vector tiles. They vary by the implementation language, supported input formats, type of interface and so on. Their main utility is to generate vector tiles from more traditional data sources like SHP or GeoJSON or to easily preview existing vector tiles. Examples: *tippecanoe*, also PostGIS via *ST_AsMVTGeom()* and *ST_AsMVT()* functions.
* *Clients*: tools that render vector tiles in the context of the device. The most widely used web-based client is probably mapbox-gl. Implementations vary by the language, offered developer tools, or by rendering contexts—those include web-based (WebGL), native to mobile and other devices (OpenGL), even terminal consoles (ASCII). Examples of web-based clients: *mapbox-gl*, *deck-gl*, *harp-gl*
* *Style editors*: tools for creating, reviewing and testing styles for vector tiles. Styles can be defined within vector tiles that are then served as base maps or they can be set by the client library when rendering.^[Styling a complex base layer across zoom levels is demanding task. Companies like Mapbox and Carto invested years of fine-tuning to their base layers that now play a significant role in attracting new customers for their services. We mention this as a proof of functional, aesthetic and economic value of quality cartography.] Examples: *Mapbox Studio*, *Maputnik*
* *Servers*: tools for serving vector tiles via API. Some server solutions provide also tile generation and styling functionalities. Examples: *Tegola*, *TileStache*, *ArcGIS Online*

In terms of cartographic visualization of big data, vector tiles bring several advantages and caveats. Using protobuff encoding, the tile size is highly compressed. It stems from the actual data content in the tile, so the feature-poor tiles are not a burden. In most cases, vector tiles have fairly small storage footprint and network bandwidth consumption, enabling global high resolution maps, fast data delivery, fast map loading, and efficient caching (for performance benchmarking and comparison to raster tiles see @giscloud2010realtime). Vector tiles are also faster to generate and they are better suited to keeping the data on the server continuously updated.

Being a vector file format, vector tiles can be styled upon request. Separating rendering from the data storage allows for creating many variant map styles based on the same data source without significant storage footprint. Client-side rendering also supports easier style customizations, e.g. font changes, label translations, showing and hiding elements, changing the layer ordering and combining data sources. Scale-based styling is also supported in many client libraries—various scaled-dependent functions can be applied on sign properties like size and color.

Vector tiles can contain attribute data which elevates the format beyond mere display technology. With raw data included, several types of interactions are possible—from basic pop-up info over points of interest towards advanced querying and filtering for in-browser analysis. Once tiles are loaded, any user-induced style changes are executed on client without additional requests to the server, which may also support offline functionality. 

For cartographer, the combination of vector tiles, WebGL-based rendering environment and capabilities of the client mapping libraries opens several possibilities:

- Vector tiles work well for both base maps and thematic interactive overlays. Furthermore, the thematic layer can be put anywhere in the layer stack, not only on the top.
- Bitmap images can be easily incorporated to the style—either as polygon textures or point symbols. Data driven styling of these bitmaps is also possible to a certain degree (Figure 29)
- WebGL-based client libraries support additional user actions like camera tilt or orientation change 
- Continuous zoom is supported as vector tiles are not fixed in size by raster resolution, so smooth impression is achieved by scaling tiles between zoom steps. This also solves the problem of fitting the mapped area to the html viewport reliably on various screen sizes and aspect ratios 
- the 3D features can be added, and the application can switch between 2D and 3D views without additional tools
- data and scale driven styling allow for fine-grained control over how is the map displayed across scales

![**Fig. 29** An example of using bitmaps in WebGL rendering environment (mapbox-gl). Three 5x5 pixel PNG images are used to define the hatched texture. The texture is applied to polygons based on data-driven rules, and clipped to the given polygon extent. Screenshot taken from <https://mapdat.uni.lu> (designed and implemented by the author).](imgs/hatches.png) 

WebGL is just one of the possible rendering contexts for vector tiles. While focus of this thesis is on web applications, we should mention that vector tiles have potential for much wider adoption—be it in cars, IoT devices, or lower fidelity peripherals. 

There are also some disadvantages and risks connected with vector tile adoption. As we mentioned earlier, vector tiles rely on a schema that defines which attributes are included, their naming and value types, the zoom levels range at which layers should appear, etc. There is no universally applicable schema, a tile layer intended for analysis would need a different definition than a layer meant as a base map. Even for general base map layers, there is a number of schemas^[Some general-purpose base map schemas: Mapbox Streets, OpenMapTiles, Mapzen Tilezen, Thunderforest] that are not interchangeable—there are different priorities of layers, different classification of roads that appear at various zoom levels, etc. When designing style for a given schema, it will most likely not be portable to a different schema.

Tiling can preclude some types of analysis that need to consider features in adjacent tiles. Though, features that are split between the tiles can be reconstructed once rendered—this is useful for example when implementing an on-hover highlight of polygon features. The rendering engine can select all bits that need to be highlighted across the displayed tiles. It needs though to be considered at the moment of tile layer creation (in mapbox infrastructure on needs to run tippecanoe with the *—generate-ids* argument so that features can be identified across tiles by mapbox-gl). As in many situations around vector tiles a tight coordination across the whole tool chain is required.

Even if we limit ourselves to web-based clients, there is a number of implementations that are not overly compatible. While some provide a domain specific language to define styles (mapbox-gl), some support writing custom GLSL shaders (deck.gl). The user experience as well as the developer experience varies. The breadth of supported styling actions is also varying (e.g. mapbox-gl does not support transparency blend modes, kepler.gl does).

As rendering happens on the client, the application performance largely depends on the power of the client hardware. Tile servers can impose some size limits on vector tile layers that, unlike raster tiles, can be bloated with large number of attributes.

Overall, we still seem to lack mature standardized tools for working with various aspects of vector tiles. Diverging schema flavors and software implementations that need to act in accordance enlarges the risk of vendor lock-in. On the positive note, there are some efforts under way to standardize vector tile metadata, server API or filtering language within OGC.^[see <http://docs.opengeospatial.org/per/> section *OGC Vector Tiles Pilot*]


## 3.5 Designing user interfaces for digital maps

In map-based web applications, it is inevitable to design some controls for users to interact with the map. @thomas2005illuminating remark that often in the visual analytic process, researchers tend to focus on visual representations of the data but interaction design is not given equal priority. While there is a growing body of research in the problem area, some practical advice for dealing with complexity in user interfaces is rather lacking.

User interface (UI) controls contribute to the overall graphic density of the application as they share the same screen space with the map. Looking at UI controls through the prism of design constraints described in Section 3.1.1, we can say that all three axes affect them, though the screen space is naturally the most significant factor (Figure 30).

![**Fig. 30** Of the three axes shown on Figure 16, the screen space is the constraint with the greatest influence on the design of map controls.](imgs/img-design_constraints-ux.png)

The UI design of digital maps is informed by the spatial and temporal density of the visualized data set. Variability in attributes translates to variability of map symbolization which then complicates both the legend and the controls. 

@norman2013design draws distinction between *affordances* and *signifiers* in product design. Affordance describes a certain relationship between objects and users—what is possible to do with an object (e.g. doors can be opened). Signifiers notify users about affordances to prevent confusion (e.g. a *pull* sticker on the doors telling us which way to open). As @norman2016living points out, complexity in user interfaces is often inevitable, and it may not be a problem if correct signifiers are presented. Users asking for simplicity in fact ask for understandability, and simple-looking things can actually be quite confusing due to missing signifiers.

In application interface design we have a tension between trying to achieve a "clean", unobtrusive look and the need to supply all the necessary information about using the application. Often just to clearly signify what items are clickable is a challenge. This information is often passed on just by changing the cursor style when user hovers over an item, though this is not an option for touch screen devices where no hovering is available. Having the interface littered with textual descriptions is deemed unsatisfactory, additional information is therefore hidden and displayed as on-hover pop-up windows (but there is no guarantee user will discover them). Another approach is to use an explanatory "wizard" at the application startup, that can be closed and revisited.

One space-saving strategy for digital maps is coupling the legends and other explanatory graphics that supplement the map with interactive controls. For example, an interface can have a color scheme legend coupled with interactive sliders to filter the categories displayed in the map. This approach certainly saves some precious screen space compared to having two separate elements, on the other hand, it is more demanding to design. Such legend needs to explain the map symbolization and at the same time clearly signify what user actions it offers.

Responsive web design is a pressing issue for complex interfaces especially if we want to preserve the same information content and affordances as in the large screen view. While there is a number of well designed data exploration interfaces for small screens,^[See for example <http://mobilev.is/> or <https://explorer.morphocode.com/map>] we usually need to trade between information content and some user comfort. 

When viewing digital maps on large screens, we expect to see all the interface controls together with the map in one view. On small screens this is hard to achieve. One possible solution is in providing a minified version of the interface. The controls then are too small to be usable, but users receive an initial global overview and can use touch gestures to zoom in and out. There are several issues with this approach. First, mobile screens have different aspect ratio, so unless we want to force users to turn the device horizontally, some layout reordering in necessary. The second big issue is in distinguishing the zoom actions within the map context from zoom actions within the context of the whole interface. It is therefore more common to change the application layout for small screens, though this often requires users to jump between the controls and the map, because they no longer fit to the screen together. To reach the controls, users need either to scroll up and down or pull some hidden collapsible panel in and out. Needles to say that this is not ideal either, mainly because users cannot directly observe how the changes made via controls alter the map view.

When it comes to research on designing map interfaces for small screens there is certainly a noticeable gap. The range of possible interaction modes on mobile devices (at least 12 types of screen gestures, gyroscope, etc.) seems to be largely unutilized. The UI on touch devices should provide feedback for user to be sure what kind of touch event was performed and whether it was registered (@willenskomer2017creating). Other parameters like physical distance from the device (e.g. when holding it vs when driving), viewing time or environmental conditions (indoors vs outdoors, day vs night) should be considered when designing the interface [@slaughter2017designing]. Principles of motion design for increasing the UI usability could as well be applied to interactions within the map field.


