# 1 Defining Big Data

In this chapter we search for what defines big data. We describe the main attitudes towards grasping the concept. Special consideration is given to the notion of spatial big data and the related works from the Cartography and GIS community.

## 1.1 Ontological characteristics

Despite the lively interest in the subject, the explanation of the term *big data* ^[Throughout the text we will treat the term as plural, without capitalization. Although there are strong arguments for "data" as singular and some authors do capitalize, we chose to match with the majority of big data related literature. This does not apply to direct citations where we preserve the original author's formulation. For arguments for singular data see @widman2014when, @nunberg2013data, for counterargument @wilson2017big] remains hazy. The term is often used without clarity, as there is no widely accepted definition to the date. Perhaps the most systematic effort in this matter by @kitchin2014data (refined in @kitchin2016makes) summarizes the key properties attributed to big data. Kitchin critically evaluates these properties and goes on to assign them a relative importance in distinguishing "big" from "small" data. He also takes care to separate the concept in itself from the accompanying social phenomena, hence he speaks of *ontological* characteristics.

Kitchin's taxonomy provides a useful starting point for our thinking of big data from the cartographic standpoint, so let us list the ontological characteristics including some of the Kitchin's comments:

- **Volume** — can be measured in storage requirements (terabytes or petabytes) or in number of records
- **Velocity** — data generation happens in real-time either constantly (CCTV) or sporadically (web search); frequency of generation can differ from the frequency of data *handling*, *recording*, and *publishing*
- **Variety** — data are heterogeneous in nature, though this property is rather weak as various levels of organization are allowed (*structured*, *semi-structured* or *unstructured*)
- **Exhaustivity** — the entire system is captured (*n=all*), rather than subset created by sampling
- **Resolution and indexicality** — data are fine-grained rather than being aggregated; data are uniquely indexical, which enables linking to other data sets
- **Relationality** — data contain common fields that enable the conjoining of different data sets
- **Extensionality and scalability** — data generation is flexible, it is possible to add or change fields easily, data can rapidly expand in size

In relation to these characteristics it is important to mention two open questions that for many people make attempts to define big data vague at best, sometimes to the point of questioning the existence of the phenomenon itself.

First, there are no quantitative thresholds that would define exactly how large the "big" volume is, how fast the "big" velocity is, and so on. Some properties would even be hard to describe in quantitative terms (for example extensionality). Other properties sound too general or vague to act as a sound defining parameter (scalability). What is more, one could extend the properties ad absurdum, for example *variety* could refer to differences in structure, origin, quality, or any other property of a data set. Such multilevel hierarchy of parameters and sub-parameters does not add to the overall comparability of data sets, especially when we consider that data generation procedures may be unique to certain domains and not found in others. Finally, many data sets lack metadata detailed enough to allow to judge all mentioned properties. It is possible that these issues will clear out with time, but parameter thresholds may as well remain blurry and ever in flux.

The second problem is that even if we had a clearly defined set of criteria, in practice we could hardly find a data set that would fit all of them. Therefore not all properties are deemed mandatory, which in turn leads to confusion and labeling almost anything as big data. To articulate the gist of the term, more work is needed on the relations of the parameters, some might be merged (resolution is a consequence of exhaustivity, indexicality enables relationality) or discarded (extensionality and scalability seem to describe the infrastructure rather than data).

Aware of these problems, @kitchin2016makes argue that *velocity* and *exhaustivity* are qualities that set big data apart and distinguish them from "small" data. We can add that these two characteristics also present the most interesting challenges to cartographic visualization of such data. If we adopt this view, we can treat the rather too simplistic adjective "big" as a short name for *generated continuously in real time and containing an unreduced set of elements*.

## 1.2 Other ways of understanding big data

In this section we briefly review the writing of authors that seek to define big data. The term itself was fist used in context of dealing with massive data sets in mid-1990s by John Mashey [@diebold2012personal], but the heaviest circulation of the term in scientific and popular media took place only in recent years. Some general tendencies keep reappearing in the plethora of works.^[For an alternative summary of definitions see @gandomi2015beyond, for bibliometric analysis of related scientific literature see @nobre2017scientific.]

### 1.2.1 Vs and keywords

Kitchin's taxonomy mentioned in the previous section is based on a review of older definitions, starting with the often-cited three Vs (*volume*, *velocity*, and *variety*) by @laney20013d. The notion of *exhaustivity* was added by @mayer2013big, concepts of *resolution* and *indexicality* came from @dodge2005codes, @boyd2012critical add *relationality*, and the qualities of *extensionality* and *scalability* were taken from @marz2012big.

Other properties attributed to big data include *veracity* (data can be messy, noisy and contain uncertainty and error) and *value* (many insights can be extracted, data can be repurposed), both brought forward by @marr2014big. One could argue that these properties are just aspects of variety, as data vary not only in type and structure but also in quality. This is can be the case for small data as well, @marr2014big argues that "the volumes often make up for the lack of quality or accuracy", which is surely debatable.

Moreover, *variability* (the meaning obtainable from data is shifting in relation to the context in which they are generated) was identified by David Hopkins in relation to text analysis [@brunnelli2011will]. @li2016geospatial name also *visibility* (efficient access to data via cloud storage and computing) and more curiously *visualistation* as big data properties.

@suthaharan2014big, dealing with early recognition of big data characteristics in computer network traffic, argues that three Vs do not support such early detection in continuous data streams. Instead he proposes three Cs: *cardinality* (number of records), *continuity* (meaning both representation of data by continuous functions, and continuous growth of size with time), and *complexity* (which is again a combination of three parameters: *large varieties of data types*, *high dimensionality*, and *high speed of processing*). One might ask why the author seeks to propose parameters in triples, even at the cost of occluding additional properties as sub-parameters. Suthaharan's approach is though interesting in observing the rate of change in parameters in real time.

Laney's 3 Vs were brought into commercial management-speak and became a slogan further powering the hype of big data. Nevertheless, it inspired a number of other authors to extend it quite creatively. For example @uprichard2013focus lists other v-words to be considered, both in positive (*versatility*, *virtuosity*, *vibrancy*...) and negative (*valueless*, *vampire-like*, *violating*...) light. @marr2014big describes five Vs of big data, @van20133v sees seven Vs, @boellstorff2015introduction propose three Rs and @lupton2015thirteen even uses thirteen p-words to describe the subject. But as @kitchin2016makes note, "these additional v-words and new p-words are often descriptive of a broad set of issues associated with big data, rather than characterising the ontological traits of data themselves".

### 1.2.2 A challenge for technical infrastructure

Several authors understand big data mainly as a management issue, which is probably due to the fact that handling large data sets is difficult. Hence, the computational difficulties of storing and processing a data set on a single machine often act as a defining measure. Consider for instance @storm2012big quoting Hillary Mason: “Big Data usually refers to a dataset that is too big to fit into your available memory, or too big to store on your own hard drive, or too big to fit into an Excel spreadsheet.” Or similarly @shekhar2012spatial state that “the size, variety and update rate of datasets exceed the capacity of commonly used spatial computing and spatial database technologies to learn, manage, and process the data with reasonable effort”. 

The problem with such definitions is in determining exactly what size is "too big to fit" and what is the "reasonable effort". The computational power of hardware accessible for personal use is constantly increasing ^[Gordon Moore's 1965 paper (reprint @moore2006cramming) stated that the number of transistors on integrated circuits will double every two years. The prediction has proven accurate for several decades and became known as *Moore's law*. The pace has slowed down with smaller transistors, which might mean we are reaching a technological limit, though opinions here vary. The overuse of the therm as a synonym of progress has been criticized as too simplistic for example by @kreye2015moores], not to mention the technical infrastructure accessible to large enterprises and governmental organizations — data center construction is steadily growing and is expected to almost double the 2016's capacity in 2021 (@statista2018data, @networking2018cisco, see also Figure 1).

At the same time, new technologies emerge to address the issue — virtualization of storage, networking, and memory make it possible to rent computational infrastructure from "cloud" providers, or to delegate workloads previously carried out by the operating system to remote platforms.^[*Cloud computing* enables companies to consume a compute resource, such as a virtual machine, storage or an application, as a utility rather than having to build and maintain computing infrastructures in house [@rouse2018cloud]. The cloud models include providing infrastructure, platform or application as a service; most popular public cloud solutions to date are Amazon Web Services, Google Cloud Platform and Microsoft Azure.] Other innovations take place in data processing algorithms, analytic engines, and in database design (a whole range of No-SQL databases as well as enablement of distributed processing in traditional databases).^[Processing and analytical frameworks designed for big data include Apache Hadoop, Apache Spark, or Apache Flink. No-SQL databases use a column, graph, document, key-value, or multi-model solution as an alternative to traditional relational database design.] Some attempts to summarize technical solutions for big data can be found in @paakkonen2015reference, or in @jin2015significance.

As we can see, the "too big to fit" definitions are highly dependent on the resources at hand, plus we need to take into account future improvements that are hard to predict. That being said, understanding the subject as *data that prevent local offline processing on common desktop in reasonable time* is a useful shorthand for judging big from "small" data. The border between local (offline) and remote (cloud-dependent) processing exists, even though it is blurry and dynamic. Cloud processing will likely be increasingly accessible, one should consider the scalability of any data-processing workflows early on. In other words, any workflow designed as a potential big data process will likely have an advantage, as design limitations may prove to be overcome harder than the technical ones. 

One reoccurring point of confusion for readers of big data related literature is mixing the characteristics of the subject (stored information) with the technologies used to process it (storage, analytics, visualization, etc.). If this is a fallacy is debatable, depending on to what degree we consider digital data independent from the surrounding technical infrastructure.^[Real world analogies may not be helpful here: for example the properties of gold are independent of the tools used to mine it. On the other hand, many forms of interaction with digital data are inseparable from the technical infrastructure.] To illustrate the difference, compare the following two definitions. Fist by @gartner2018what:

*Big data is high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation.*

The second by @gantz2011extracting defines big data as:

*A new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data by enabling high-velocity capture, discovery, and/or analysis.*

The understanding of big data as an asset prevails, though the second type portraying big data as an ecosystem is not uncommon (e.g. @demchenko2014defining or @olshannikova2015visualizing). Eventually, this division may lead to dual understanding of big data in narrow sense as a fuel or raw material and in broad sense as an ecosystem, architecture, or  framework. A good example of broader thinking is @demchenko2014defining that proposes a "Big Data Architecture Framework" comprised of big data infrastructure, big data analytics, data structures and models, big data life cycle management, and big data security.^[This is close to holistic definitions discussed later in this chapter, though these tend to be less confined in technology realm, mixing in procedural aspects and wider societal implications.] 


### 1.2.3 Showing example sources and quantities

A very common description of big data goes along the lines of "I will give you some numbers and you will get what I mean". Such writing may not provide an exact understanding of the concept, but can give us some context about the scale of things. Doubtlessly the mass of retained data is growing, as @mcnulty2014understanding puts it, "90% of all data ever created was generated in the past 2 years" (that was in 2014). In a notable attempt to estimate the World's overall data generation between 1986 and 2007, @hilbert2011world claim that more then 300 exabytes ^[1 exabyte = 1 000 000 000 gigabytes] of stored data existed in 2007 (for the methodology of reckoning see @hilbert2012measure). The key insight is the growing domination of digital technologies accounting for the majority of the annual growth after year 2000. More recent accounts report on machines potentially capable of processing brontobytes ^[1 brontobyte = 1 000 000 000 exabytes] of data [@bort2014there].

Increasing the storage capacity itself does not speak of any qualitative change in what is stored — archives and libraries could be described as big piles of small data. Under certain circumstances, new quality can arise from increased quantity, for example as @norvig2011unreasonable points out, an array of static images projected at a sufficient frame rate creates an illusion of movement, and hence the new medium also known as film. Multiplication of an old medium creates a new one. The remaining question is: under what conditions this change of essence arises, and if such thing occurs or will occur in case of big data. The cartographic version of this question would be: *would a digital map based on big data (fast and n=all) be essentially different from digital maps based on static and sampled data sources?*.

![**Fig.1** Comparison of the World's estimated data storage capacity between years 1968 and 2007 (modified after @hilbert2011world) and the expected storage capacity of large scale data centers in the period from 2016 to 2021 (modified after @networking2018cisco).](imgs/img-storage-capacity.png)

Rather than putting up to a gargantuan task of counting the mass of all existing data items, authors use the available statistics related to operations of large companies [@kambatla2014trends; @mcnulty2014understanding; @marr2014big and others]. For example, Facebook was said to process 10 billion messages, 4.5 billion button clicks and 350 million picture uploads each day [@marr2014big]. It goes without saying these numbers are outdated and certainly outgrown today. Other companies prominently mentioned in context of big data are Google, Wallmart, or Amazon. This connection is justified, as these companies have put customer data analytics to the core of their businesses, and to a degree also fuel research in the field. Social media, web search date, web browsing data, online or offline shopping patterns, mobile devices, sensors and large scientific projects are mentioned as generators of big data.

Market value is another metric of interest. For example @kayyali2013big modelled a reduction in health care costs of 12 to 17 percent thanks to emerging big data related initiatives in USA. On the other hand, the use of poor data is also estimated to have vast impacts on businesses, mainly in form of unrealized opportunities [@mcnulty2014understanding]. Another financial aspect is the cost of creating and maintaining big data itself — it is sound to remind that apart from all the promise, big data also have the potential to cost unlimited amounts of money [@fischer2015why].

Type of data source is another classification property. Authors distinct "traditional" ways of collecting data from the new, technology-powered sources. The definition of big data then comes as simple as "data coming from these new sources". The United Nations Economic Commission for Europe proposed a taxonomy that recognizes three main sources of big data [@unce2013]:

- *Social Networks (human-sourced information)*—this information is the record of human experiences
- *Traditional Business systems (process-mediated data)*—these processes record and monitor business events of interest
- *IoT (machine-generated data)*^[Internet of Things (IoT) can be described as a vision of a network of devices, vehicles and home appliances that can connect, interact and exchange data. Similarly to big data, there are manifold definitions of the concept, for overview see @atzori2010internet]—information is derived from sensors and machines used to measure and record the events and situations in the physical world

Data sources labeled as "big" differ from traditional sources such as surveys and official administrative statistics — @florescu2014will and @kitchin2015opportunities closely examine those differences as well as the potential for big data to extend the official statistics. The authors point out that volume is not a distinctive property as governmental offices also store large volumes of data. What makes the difference is that classical data sources have the statistical products and by-products specified beforehand, while big data tend to be reused beyond the original intent. On the other hand, big data sources tend to be volatile and unstructured, therefore their representativeness is harder (if possible) to assess.

The estimation in Figure 1 could not have predicted the spread of COVID-19 pandemic. According to International Data Corporation (IDC), more than 59 zettabytes^[1 zettabyte = 1 000 exabytes] were to be created, captured, copied, and consumed around the world in 2020. The COVID-19 pandemic contributed to this figure by causing an abrupt increase in the number of work from home employees and changing the mix of data being created to a richer mixture including video communication and consumption of downloaded and streamed video. IDC also measures the amount of data created and consumed in the world each year. The ratio of unique data (created and captured) to replicated data (copied and consumed) is roughly 1:9, and it is expected to move to 1:10 by 2024. This trend was also fuelled by increased consumption of replicated data due to COVID-19 pandemic [@idc2020global].


### 1.2.4 Metaphors

Metaphors rely on a notion of analogy between two dissimilar things, but can also turn into independent verbal objects, aesthetically appealing but not overly revealing. Despite that, we should not ignore metaphoric accounts as they contribute to the mythology surrounding big data that reflects what many people expect. 

@puschmann2014big identified two prevailing ways of imagining the subject: big data seen as a *natural force* to be controlled and as a *resource* to be consumed. 

The utilitarian mindset comparing digital world to excavation of valuable minerals in far from new. Though it is tempting to pursue this analogy further. For example, how to estimate the ratio of valuable information to "debris", and shouldn't such estimation be done before any data "mining" endeavour? The value of real-world analogies may be in provoking some common-sense reasoning often missing in visionary proclamations. 

For example, quoting @mayer2013big: "Data was no longer regarded as static or stale, whose usefulness was finished once he purpose for which it was collected was achieved [...]. Rather, data became a raw material of business, a vital economic input, used to create a new form of economic value. Every single data set is likely to have some intristic, hidden, not yet unearthed value...". So what is yet to be unearthed is not the data itself but new way of using it.

As @lupton2013swimming notes, by far the most commonly employed rhetorical descriptions of big data are those related to water or liquidity, suggesting both positive and negative connotations. For example @manyika2013open argue for unlocking data sources to become "liquid" in a sense of open and free-flowing, while, of course, keeping privacy concerns in mind — what is liquid is also susceptible to unwanted leaks.

Big data have also been described as a *meme* (a unit of cultural transmission) and as a *paradigm* (a set of thought patterns), in both cases not without concerns. @gorman2013danger explores big data as a technologic meme: "[t]he reductionist methods of understanding reality in big data produce new knowledge and methods for the control of reality. Yet it is not a reality that reflects the larger society but instead the small minority contributing content." To @graham2013geography "big data could be defined as representing a broader computational paradigm in research and practice, in which automated algorithmic analysis supplants domain expertise".

Of course, big data descriptions are not limited to verbal form, visual means can be much more expressive and informative — not a surprising claim to be found in a thesis on visual analytics. We will discuss cartographic tools later, here we can mention artistic renderings that employ more free-form visual analogies. We should distinguish pursuits like *infographics* that are close to graphic design (for good overview see @klanten2010data or @lima2011visual) from artistic projects that use data as a raw material and don't aim to convey information or comfort to general user's cognitive expectations (like some projects at @creative2018). From the cartographer's standpoint, aspects of visual art can be inspiring (graphic quality, employment of computation and rendering software, creative uses of interaction and animation), though artistic means are often too other-worldly to be transposed. Without referring back to the source phenomenon, data-driven art can be confused with the generative art that uses artificially generated data rather than any existing information.


### 1.2.5 Holistic accounts

Multifaceted phenomena tend to provoke descriptions that narrowly focus on some components, ignoring other parts as well as relationships between them. Experts of different specializations notice aspects of phenomena that are close to their research interests and priorities, cross-disciplinary definitions then try to combine these views to paint the full picture. Naturally, listing holistic accounts will include topics already mentioned, therefore pardon some repetition.

For instance @murthy2014big prepared a taxonomy of big data comprised of:

- *data* — with various levels of temporal latency and structure 
- *compute infrastructure* — batch or stream processing
- *storage infrastructure* — distributed, sql or nosql databases 
- *analysis* — supervised, semisupervised, unsupervised or reenforcement machine learning
- *visualization* — maps, abstract, interactive, real-time 
- *privacy and security* — data privacy, management, security

As another example, @boyd2012critical define big data as a "cultural, technological, and scholarly phenomenon that rests on the interplay of":

- *technology* — maximizing computation power and algorithmic accuracy to gather, analyze, link, and compare large data sets
- *analysis* — drawing on large data sets to identify patterns in order to make economic, social, technical, and legal claims
- *mythology* — the widespread belief that large data sets offer a higher form of intelligence and knowledge that can generate insights that were previously impossible, with the aura of truth, objectivity, and accuracy
 
As the two taxonomies above illustrate, there are many ways to slice a cake. The fate of overreaching definitions is that they are often too intricate to explain the phenomena crisply, yet they are never complete as there is always a point of view that hasn't been included yet. So here we arrive at a trade-off between preciseness of definition and its practicality. One way out of this is simply in rejecting the view of big data as a singular phenomenon. Big data is then a non-specific covering term that could mean different things to different people. As @helles2013making observe, "[d]ata are made in a process involving multiple social agents — communicators, service providers, communication researchers, commercial stakeholders, government authorities, international regulators, and more. Data are made for a variety of scholarly and applied purposes [...]. And data are processed and employed in a whole range of everyday and institutional contexts." The process, the actor, the purpose and the context then determine what big data "is" in that given constellation. 

We can conclude the section on holistic approaches with a historical view that is rarely taken in commentaries on the nature of big data, probably because the perceived novelty of the concept. For @barnes2013big "[b]ig data has been made possible because of the particular conjuncture of different elements, each with their own history, coming together at this our present moment. But precisely because these different elements have a history, the issues, problems and questions that were there in their earlier incarnation can remain even in the new form". We can add that some issues can get worse in the new incarnation and totally new set of problems can arise. For example, as @mayer2013big note, current anonymization techniques can be rendered ineffective as combining several "data traces" of online activity can still identify the person. Or, as @taleb2012antifragile realizes, if big data come with too many variables but with too little data per variable, it becomes nearly impossible not to find high but spurious correlations, which can tempt researchers to cherry-pick the results that "support" their hypothesis. Considering wider implications of technology can potentially make such unintended effects less surprising, which is certainly a virtue of holistic thinking.


## 1.3 Spatial big data

Apart from the general definitions mentioned above, there have also been field-specific efforts to contextualize big data. The fields include governance [@crampton2015collect], journalism [@lewis2015big], ecology [@shin2015ecological], social sciences [@ovadia2013role], business administration [@wamba2015big], urban studies [@thakuriah2017big], learning analytics [@wilson2017big], education [@kabakchieva2015big], health informatics [@herland2014review] and doubtlessly many others. Authors consider the existing data processing and analytical practices in their respective disciplines in light of possibilities created by big data. Some expect forthcoming changes such as enrichment in available methods (e.g. analysing social networks in epidemiology), others analyze the adaptability of currently used processes to conditions of higher data load. With some generalization, the overall mood of these works seems to be welcoming towards big data as a possible toolbox extension, though doubting that the core scientific methods could be deeply altered by it. When it comes to defining big data, field-specific accounts use one or more of the aforementioned definitions (by *keywords*, *constraints*, *examples*, *metaphors* or combination of all in a *holistic* description). 

Within geography, @kitchin2013big highlights possible opportunities, challenges and risks posed by big data, encouraging geographers to engage in big data related case studies. He also lays some groundwork for definitions he later developed into ontological characteristics cited at the beginning of this chapter. @gonzalez2013big understands big data predominantly as a rich set of observations of intricate and nested social life that can improve theories of human geography, for example by exposing diversity that would otherwise go unnoticed in scientific models. @barnes2013big reminds us of the so called *quantitative revolution* in geography (starting from 1950's) that besides bringing many good to the discipline has also been criticized on various levels. Some of this critique, Barnes argues, "continue[s] to apply to the *über* version of the quantitative revolution that is big data". For @goodchild2013quality geography provides a distinct context for discussion about what kinds of science might be supported by big data. He is also concerned with the potential for building rigorous quality control and generalizability into big data operations, because so far "instead of relying on the data producer to clean and synthesize, in the world of big data these functions are largely passed to the user". We could go on much further with how geographic thought internalizes big data, those interested in the topic may refer to @thatcher2018thinking.

Cartographers and GIS practitioners like to say that 80% of all data is geographic, and even though such claim is hard to prove^[see @morais2012phrase for discussion and @hahmann201180 for a validation attempt], few would doubt that spatial reference can unlock additional value, if only as a platform for joining otherwise un-joinable data sets. Much of data in the world is or can be georeferenced, which underlines the importance of geospatial big data handling.

Cartography and geographic information science have both developed distinct and elaborate notions of data in general. Scientists and practitioners from these fields are in good position to contribute to the way big data is understood and utilized, given their focus on the space as a unifying factor and with visual analysis being at the core of their practice. For these reasons, we will first take an aside to briefly outline how cartography and geoinformatics conceptualize spatial data, before moving on to how the disciplines contended with the adjective big. We consider the following points important:

* Data describing spatial phenomena used in GIS are traditionally divided into *spatial* and *non-spatial* (thematic, attribute) components. Spatial component holds information on location and geographic extent of an entity and can be thought of as a geometry that is visualized on a map or used for spatial analysis (spatial querying, overlay algebra, network analysis, etc.). Attribute information can be used to set visual parameters of geometries on a map as well as in spatial analysis. Visualising attributes lets us observe the variability of a phenomenon across the area of interest. @andrienko2006exploratory offer more general view of data as a correspondence between referential and characteristic components. Referential components (or referrers) are described as independent variables — mostly employed referrers are *location*, *time* and *population*. Referrer or a combination of referrers provides context and unique identification for dependent variables — attributes.

* Literature distinguishes two approaches to representing the spatial component of data in GIS: *object-based* and *location-based* [@peuquet1994s]. The object-based approach arranges spatial and non-spatial information into discrete geographic objects (features). In the location-based approach, attribute information is stored relative to specific locations. With this approach, a territory is divided into same-size elements that represent locations to assign attributes to. Object-based approach is manifested in *vector data model*, location-based approach corresponds to *raster data model*. In vector data model objects have either point, line or polygon representation. Objects are usually grouped into layers of same theme and geometry type. In raster data model, representation is defined by the size of the element (almost always being a rectangular pixel). Raster model suits better for displaying spatially continuous phenomena, whereas vector model tends to be more appropriate for discrete objects, though reverse situation is not uncommon and transformation between models is a frequent practice.  

* Attributes are typically distinguished according to the levels of measurement introduced by @stevens1946theory: *nominal* (named variables), *ordinal* (allow ordering), *interval* (allow measuring difference), and *ratio* (having natural zero). @jung1995knowledge proposed an alternative classification more tailored to spatial data handling: *amounts* (absolute quantities), *measurements* (quantities requiring units of measurement), *aggregated values* (amounts or measurements summarized by area), *proportional values* (normalised by a fixed value), *densities* (divided by corresponding area), *coordinates* (position in some coordinate system). 

* The temporal aspect of a phenomenon includes the existence of various objects at different moments, and changes in their properties (spatial and thematic) and relationships over time [@andrienko2006exploratory]. Including the temporal aspect into the data model is problematic as it is treated separately from spatial and attribute components despite having influence on both. For the attribute part, the time changes can be stored by adding table columns with new values. However, changes in the spatial component are not easily stored, which complicates linking the past forms of geometries with corresponding past values of attributes.^[This is most pressing when handling spatial data in discrete files (e.g. in Shapefile or GeoJSON formats). Using versioning systems like Git, which has become incredibly popular for handling software source code and text files, is not suitable for spatial data files as these often exceed repository size limits (though there is a project attempting to solve this called *geogig* <http://geogig.org/>). Handling spatial data within relational database provides more options for spatial data versioning, also there is a range of database project specialized on storing time series like InfluxDB or TimescaleDB.] Incorporating flexible time changes into GIS data model remains a challenge for spatialization of big data.

* Spatial component of data may be displayed at various scales. The scale along with the purpose of the map influences the level of comprehensible detail in displayed geometry. Cartographic generalisation is the process of adjusting the map geometry to the spatial scale in which the area is displayed. This goes beyond mere simplification, as factors as *highlighting the important*, *maintaining the object relationships* and *preserving the aesthetic quality* come to play. The dynamic change of scale comes naturally to users of digital interfaces, but generalization is hard to automate as it involves complex reasoning and considerations of object relationships that span through the strict topic-based separation of layers common in spatial data sets.^[For more on efforts in automated generalisation see for example @burghardt2016abstracting] The same phenomenon can be studied at various levels of detail even without changing the scale of the map. Some spatial data sets, such as administrative units, exhibit the nesting property that allows to vary the granularity of the displayed spatial pattern.

The above summary is inevitably simplistic as there are many other research areas in cartography and GIS that are relevant to big data efforts. Some will be touched on later in the thesis, others are unfortunately out of its scope. One such case for all is spatial imagery that is an example of truly big data source that is inherently spatial. "Big" in this case means unprecedented spatial, temporal and spectral resolutions brought about by improvements in global monitoring systems.

In light of big data advent, authors form spatial fields consider what difference does it make to conceptualize a specifically *spatial* big data as opposed to big data per se. Is spatial big data a subset or an extension of big data? From the GIS point of view there are two ways of understanding spatial big data: either as (a) *adding spatial reference to big data* or as (b) *adjusting the current spatial data models and processes to higher data load*. We can say that these two approaches arrive at the concept of spatial big data form the opposite sides, in the first case the path is *from big data to spatial big data*, whereas in the second case it is *from spatial data to spatial big data*.

Authors from the first group use some of the previously mentioned definition styles. For example to @jiang2017spatial, spatial big data refer to "georeferenced data whose volume, velocity, and variety exceed the capacity of current spatial computing platforms". This combines definitions by V-words and computational difficulties. @lee2015geospatial, on the other hand, combines definition by constraints and by example. In this context we can mention some early critique that condemned narrow understanding of big data, aiming mainly at analyzing geotagged social media content (labeled as "burger cartographies" by @crampton2013beyond and @shelton2017spatialities). As @leszczynski2016introduction note, social media content covers just a limited facet of the data productions, presences, and practices that fall under spatial big data.

Representing the second group, @yao2018big recognizes five categories of spatial big data (while admitting some intersections): *remote sensing data*, *large data from surveying*, *location-based data from mobile devices*, *social network data*, and *Internet of Things (IoT) data*. Yao and Li then focus on a subgroup they name *big spatial vector data* (BSVD), and provide a comprehensive survey of techniques applicable for managing such data. In short, adjusting the vector spatial data model for distributed storage impacts how the data is indexed^[Spatial indices are used to optimize retrieval of spatial data from database. They decrease the time it takes to locate features that match a spatial query.] and queried for processing and application. @yao2018big also provide an overview of other authors' approaches to thinking about GIS in the era of big data. 

In context of transportation, @shekhar2012spatial distinguish between *traditional* and *emerging* spatial big data. "Traditional" stands for topological vector data representing transportation infrastructure, "emerging" represents sensor and positional data from large number of vehicles — termed as *spatio-temporal engine measurement data*. @shekhar2014benchmarking call for performance testing of the existing and new algorithms to assess proper comparison between spatial big data processing techniques.

To @li2016geospatial, the main sources of spatial big data are in *volunteered geographic information (VGI)*^[VGI is defined as "the harnessing of tools to create, assemble, and disseminate geographic data provided voluntarily by individuals" [@goodchild2007citizens]. This description fits for example the contributions to the Open Street Map project very well, but is less applicable to social media, where users are more likely indifferent to their data being collected.] and in *geo-sensor networks* (with extended understanding of sensor including CCTV and mobile devices). @li2016geospatial also touche on a wide range of topics, ranging from quality assessment (big data properties invalidate the current error propagation methods) to the importance of parallel processing of data streams (where the advantages of functional programming languages are recognized). @van2014spatial mention *Internet of Things* a main future source of big data — here understood as a sum of sources from "smart" devices. Geospatial technologies are considered a binding principle that would eventually help to meaningfully combine data from devices to bring about the rise of smart city.^[Smart city is a concept of urban area that uses digital information to make more efficient use of physical infrastructure, engage effectively with people in local governance, and respond promptly to changing circumstances. For more information see @mclaren2015sharing] 

In relation to big spatial data processing, we should mention the work of Bin Jiang that is somewhat isolated from the categories mentioned above, but provides interesting thought on how the current GIS processes could be altered. @jiang2018spatial recognizes the following dichotomies and potential paradigm shifts: 

- *Gaussian* vs *Paretian statistics*^[Named after Vilfredo Pareto who more than century ago noticed that in 20% of people in Italy owned 80% of land. The ratio of 20% of causes leading to 80% of consequences has been observed in many systems, though the distributions can be far more uneven, like that 99% of Internet traffic is attributable to 1% of sites [@taleb2012antifragile].]—the first suits better for sets with elements of more or less similar size and expects normal distribution, the latter is based on the notion of far more "smalls" than "larges" and expects Poisson or other fat-tailed distribution. 
- *Tobler's law* vs *scaling law*—complementary concepts, where the first expects inverted proportionality between the distance and similarity of objects, which is often justified locally but does explain the abrupt spatial heterogeneity brought about by fat-tailed distributions. Scaling law, as Jiang formulates it, accounts for uneven distributions across scales. 
- *Euclidean* vs *fractal (natural) geometry*—the first is needed "to measure things", the second can help us to "develop new insights into structure and dynamics of geographic features". [@jiang2016fractal]
- *data quality* vs *data character*—Jiang defines data character mainly as topological relationships between meaningful geographic objects (e.g. connectivity of street network), which for many purposes can be more important than the precision of geometric primitives.
- *mechanistic thinking* vs *organic thinking*—the latter promotes the understanding of geographic space as a living structure shaped by the interaction of elements at various scales.

Though some of Jiang's distinctions may seem unclear and he is silent about how to incorporate organic approaches to GIS data models, he recognises that big data would be vital in changed GIS practices. For example in his notion of natural cities, social media data are used to define the "natural" extent of the city, so a city is understood more as a bottom-up emergence rather than a top-down administrative demarcation.

As we have seen in this section, geospatial authors rarely diverge from general definitions of big data, but when it comes to spatial big data, they consider the topic from the standpoint of pre-existing theory generated in the field. This conscious assessment of current data models and processes and possible creation of new ones can bring interesting developments in the future. 

The potential role of cartography will be examined in more detail later in the thesis, here let us briefly go over the big data properties listed at the beginning of the chapter to see the most obvious cartographic concepts and challenges that could possibly tie to them:

- *Extensionality & Indexicality* — spatial reference in itself is a unifying platform to combine data from various sources and  map is a proven tool to explore spatial interrelations. From the perspective of data processing workflows, spatial extensionality presents a task for geocoding services to spatialize previously unchartable data. From the map design perspective the task is to support recognition of spatial co-ocurrence in dense displays. Indexicality is a natural prerequisite for thematic mapping.
- *Volume* — from the cartographic standpoint, the number of records is the most interesting measure of volume (compared to storage size or attribute length). Extensive volume does not necessarily present a problem for effective visualization, especially if it plays out in the attribute space and the spatial reference is static. Maps that use the right visualization methods naturally support information compression and clarification.
- *Scalability & Resolution* — adjusting visualization to different scales both in terms of spatial extent and in terms of data load is a domain of cartographic generalization. Effects of varying time, space, and attribute resolution on displayed information has long been studied within cartography.
- *Variety* — digital mapping requires some structure in data, though it is not a requirement for attributes as long as the spatial reference is valid. There is though a gap in incorporating unstructured data to digital mapping, for example in adjusting metadata profiles (e.g. by moving from hierarchical classification to messier but more flexible methods like tagging), or in determining data quality from spatial context. Cartography is in a good position to search for ways to combine structured and unstructured data in a meaningful way. 
- *Velocity & Exhaustivity* — these parameters will be dealt with in more detail in chapter 3, they relate to a large set of topics internal to cartography. Velocity is connected for example with the rate of visualization update and the time span of the depicted theme. Cartography is ideal for depicting time-space regularities and relationships within and between data sets. Exhaustivity then projects into the problem of graphic fill and the need to tailor cartographic visualization to human cognitive capabilities.

It is not within the scope of this thesis (and within the author's powers) to consider all areas where cartography and geographic information science may be impacted by big data. The whole project of GIS might need to to be rethought again, but this is not unprecedented. From the desktop GIS (1960s) to the web GIS (1980s), and the distributed GIS (1990s), to the cloud GIS (2010s), it is well known that the development of GIS is greatly influenced by computer science technology @[yang2010geospatial]. Another turn might come as a response to big data.


## 1.4 Assessing impacts, threats and opportunities

Big data can also be described indirectly by the impacts (real or imagined) they have on the society. For some authors, the debate around the definition of big data may be dismissed as unproductive. The popularity of the term itself may diminish like many other buzzwords that went through the technology hype cycle.^[Hype cycles describe how expectations from emerging technologies evolve with time. Stages in the cycle are:  *innovation trigger*, *peak of inflated expectations*, *trough of disillusionment*, *slope of enlightenment*, and *plateau of productivity*. The expected duration of the cycle differs per technology, and some technologies may not reach productivity in the foreseeable future. Hype cycles are a construction of the Gartner consultancy that issues regular reports, see for example @gartner2018] Many ideas in the IT industry exist under changing or concurrent names, and big data have indeed a lot in common with concepts such as *data mining*, *business intelligence* or *visual analytics* to name just a few. For many the term is just too underdefined and overused. But we should not forget that even though the technological industry is largely fashion-driven, its societal impacts are real, even though at times unevenly distributed. 

It is beyond the scope of this thesis to consult all of these impacts in detail (for such discussions see @bollier2010promise, @swan2015philosophy, or @mayer2013big), though the puzzle of big data definitions would miss some important pieces without touching on some of the consequences in *scientific inference* and *knowledge-based decision making* — the areas cartography aims to support. Closely related are the issues of *surveillance* trough big data and the *emerging digital divides*.

The scientific reflection of big data revolves around the question if the advances in data acquisition change the definition of knowledge. The anticipated mindset changes voiced in @mayer2013big can be summarized into the following points:

- Reduced need for sampling with accessibility of n=all data sets
- Loosened requirements for exactitude as minimized sampling errors would leave room for more relaxed standard for measurement error (a bit of accuracy sacrificed in return for knowing the general trend faster)
- Departure from the search for causality: "big data is about *what* not *why*." Multi factor correlation with large data enables decision making even without understanding the mechanisms behind the relationship. In words of @anderson2008end: "Who knows why people do what they do? The point is they do, and we can track it and measure it with unprecedented fidelity. With enough data, the numbers speak for themselves."

Correlation does not necessarily imply causation, though if we do not aim for understanding the phenomenon and just want to obtain some instruction for action, correlation might be enough to provide some backing. For the optimistic commentators, this abandoning of theory can open doors to iterative experimentation and building of useful heuristics that are not burdened by preconceptions and biases of our thinking. To others, this sounds scary at best, as such naive data appreciation can dangerously rationalize incompetent guesswork. As @silver2012signal puts it, most of the data is just noise, as most of the universe is filled with empty space.

Claims about objectivity and accuracy of big data are criticized as misleading, as there is almost always a need for human interpretation. For such interpretation bigger data are not automatically better. For example, multidimensionality of data sets can increase the probability of spurious correlations. Data-driven rhetoric can be suspicious as it allows decision makers to evade responsibility or to ignore alternative solutions. Furthermore, in decision making under opacity, over-reliance to historical records can catch us ill-prepared for unprecedented large scale events. Despite the air of progress and innovation @barnes2013big sees big data as an inherently conservative project: "By utilizing the numbers as they are given, big data is stuck with what is rather than what should be". In both innovation and risk management, *imagination* is a vital virtue that big data cannot supplant. 

The proposition of theory-free science that uses the powerful exploratory potential of big data to opportunistically exploit new avenues as they appear sounds promising to many. But hypotheses are inevitably formed and also can be flexibly modified during the research process. In words of P. Gross: “In practice, the theory and the data reinforce each other. It’s not a question of data correlations versus theory. The use of data for correlations allows one to test theories and refine them” [@bollier2010promise].
          
Apart from the problem of naive assumptions (like *more is better* or *big data = smart data*), there is a philosophical concern of *representational authenticity* [@swan2015philosophy] — the degree to which the representation (in this case big data) corresponds to the represented as well as how to measure this correspondence. Any mode of interacting with big data is a representation and not necessarily reality, and the reality gap may be so big that data, however big, might not be relevant [@siegfried2013big]. In words of @uprichard2013focus: "If we are creating a mess by generating so many haystacks of big data that we are losing all the needles, then we need to figure out a different kind of way of doing things, as we cannot sew new cloth without any needles. Whatever else we make of the ‘big data’ hype, it cannot and must not be the path we take to answer all our big global problems. On the contrary, it is great for small questions, but may not so good for big social questions."

The critical accounts do not negate big data as a tool, rather they dismiss the shallow reflection of its usage. Such discussions can strip bare our conceptual gaps and turn our attention to the right direction. Big data can then be leveraged to support an optimistic goal, for example to create *overreaching predictive mathematical frameworks for complex systems* [@west2013big]. Big global issues in ecology, pandemics or financial markets exhibit traits of complex systems.^[Complex system's collective characteristics cannot be easily predicted from underlying components: the whole is greater than, and often significantly different from, the sum of its parts. A city is much more than its buildings and people. Our bodies are more than the totality of our cells. This quality is called *emergent behavior* [@west2013big]] "The trouble is, we don't have a unified, conceptual framework for addressing questions of complexity. We don't know what kind of data we need, nor how much, or what critical questions we should be asking. 'Big data' without a 'big theory' to go with it loses much of its potency and usefulness, potentially generating new unintended consequences" [@west2013big]. All things considered, "[...] the arrival of Big Data should compel scientists to cope with the fact that nature itself is the ultimate Big Data database. Old style science coped with nature’s complexities by seeking the underlying simplicities in the sparse data acquired by experiments. But Big Data forces scientists to confront the entire repertoire of nature’s nuances and all their complexities" [@fan2014challenges].

The aforementioned discussions highlight the lock-step evolution of science and technology, and the strong reflection and self-correcting mechanisms inherent to science that are set in motion when innovation is accompanied with some troubling signals.^[For other examples of such reflections see @lipton2018troubling, @norvig2012warning] In broader society we also need such a reflection of new realities created by big data and the accompanying ethical issues.

One set of ethical issues revolves around data collection without giving people the choice to opt out, or without asking for explicit and informed consent. Even if consent is solicited, for users it is often impossible to audit the secondary uses that the collected data will cater to. It is hard to track what additional sources and analytical engines will be applied on collected user data and what third parties will get hold of it through reselling. At the time of writing, the legislation to address these issues is catching up^[Legislation varies around the world, for European Union, the General Data Protection Regulation (GDPR), which governs how personal data of individuals in the EU may be processed and transferred came into being in 2018. For an overview of digital privacy rules see <https://europa.eu/youreurope/citizens/consumers/internet-telecoms/data-protection-online-privacy/index_en.htm>.], but it is unsurprising that it lags behind the new kinds of abuse stemming from the extended scope of personal information that can be collected. Even with legislation in place, the enforceability is low and even learning about misuse is difficult without help from whistleblowers.

Furthermore, the anonymization methods may no longer work as combining digital traces from several sources allows for re-identification of an individual. Another topic is the ability of user to access the collected data, either to use it for own self-analysis, or to issue its removal (though how to verify it has actually happened?). In an alternative vision of big data economics, individuals may gain power to sell their data themselves or through intermediaries.

Penalties based on propensities — a short description of the following concern: using surveillance and predictive analytics, there will be a possibility to issue preventive penalties for offences that did not happen yet, solely based on individual's observed tendencies [@mayer2013big]. It is a fact that the technical infrastructure for close personal scrutiny and behaviour enforcing has been already implemented at the scale of a warehouse [@head2014worse] as well as a country (most (in)famously in China), with little room for individuals to object. At the time of writing, the global pandemics of COVID-19 created a justification for public scrutiny at unprecedented levels. On the other hand it also laid bare the inability of some state apparatuses to use their data stacks for meaningful action.  

Social media are a platform that apart from positive effects also created unexpected avenues for illicit actions, sometimes at a scale that can shake up a state. Fake news, troll farms, data breaches used to manipulate elections are all examples of the *weaponization* of the platform. Data literacy is then one of the prerequisites for defence against malicious effects. In words of @d2017creative: "[...] although there is an explosion of data, there is a significant lag in data literacy at the scale of communities and individuals. This creates a situation of data-haves and have-nots. But there are emerging technocultural practices that combine participation, creativity, and context to connect data to everyday life. These include citizen science, data journalism, novel public engagement in government processes, and participatory data art." 

The definition of big data is elusive perhaps also because the majority of involved actors, being positioned in the business world, is more focused on building productive big data ventures without much conceptual attention to the subject itself. Then of course, the underlying technologies become a subject of marketing which often uses inflated overstatements based more on expectations than on reality. So far there is no settled consensus around big data definition in the academia either, but as @kitchin2016makes predict, the "genus" of big data will probably be further delineated and its various "species" identified. The question is if then such an umbrella term will be necessary. Anyways, the lack of common ground in understanding what big data is (illustrated by this chapter) may be a good predictor of the term's future relevance. Problems with the definition is exactly what leads @davenport2014big to predict "a relatively short life span for this unfortunate term”. Indeed, the peak of big data excitement took place around 2014, and the hype moved towards *machine learning* that gets inflated nowadays. On the other hand, the number of researchers and practitioners willing to invest their time in big data related endeavours is relatively high^[*Journal of Big Data*, *Big Data Research*, *International Journal of Data Science and Analytics*, *Big Data & Society*, *Big Data* and *Big Data Analytics* are examples of scientific journals tracking cross-disciplinary efforts in the field.], which sheds some positive light on the future vitality of the concept. 

To @mayer2013big big data stand for "the ability of society to harness information in novel ways to produce useful insights or goods and services of significant value". Focus of this definition is on the real-life impacts that are likely to stay even when the big data hype is over. Even if we dismiss the term as a buzzword, the fact that more digital information gets created and can be linked more easily has many implications on the way we live. Together with that, there are changing attitudes to putting data to work. In the next chapter, we will look at how we can derive insight from big data as well as on the possible role for cartography in these endeavours.

