# Philosophy of Big Data

@swan2015philosophy
-------------------

Overview tables (point to notable references):
-- qantitative / qualitative formulations of information
-- field of concern with information (gneral, ontology, logic, ethics, aesthetics, history, unifications)

Philosophy of information, concerns around:

**Method**:
-- possible fallacies (more is better, big data = smart data)
-- is there still a need for hypothesis? Science could become theory-free without pre-specified hypothesis (and assumptions) guiding the research, or hyptheses generated or changed in progress of research. But could be too unguided and lead to nothing. See @pietsch2015aspects
-- causality -- problems beyond direct causal relation of one cause producing one effect

**Tools**:
-- technoscience -- technology-driven science conducted in technological milieu
-- lock-step evolution of science and technology, changing rapidly
-- the focus is on the ongoing enabling capacity of phenomena, not their behaviour in one fixed situation (hypothesis-less pattern observations in a way?)

**Concepts**
-- conceptual issues (table + literature): causality, quality, security, big data, uncertainty
-- **representation** -- big data have no inherend holistic form directly accessible to humans, always has to be accessed via representation that captures the underlying phenenomenon.
-- repticity (representational authenticity) -- the degree to which the representation corresponds to the represented (onthology) as well as how to measure this correspondence (episthemology)

Philosophy of big data:
-- Kant: perception without conception is blind, and conception without perception is empty
-- different kinds of blindness in big data: conceptual and perceptual
-- Any mode of interacting with big data is representation and not necessarily reality.

Big Data and complexity science:
-- optimisitc: overreaching predictive mathematical frameworks for complex systems @west2013big
-- pesimistic: map-territory modelling problem -- the reality gap is so big that data howerver big might not be relevant @siegfried2013big


@liu2014philosophical
----------------------
-- big data = second data revolution
-- Quaternion of data-theory-observation-phenomenon (TODO?)

@bollier2010promise
-------------------
-- some citations about usefulness of therory -- might be useful
-- Using correlations as the basis for forecasts can be slippery for other reasons.  Once people know there is an automated system in place, they may deliberately try to game it.  Or they may unwittingly alter their behavior.

**how to craft theories in age of big data?**
-- Patrick W. Gross, Chairman of the Lovell Group, challenged the either/or proposition that either scientific models or data correlations will drive future knowledge. “In practice, the theory and the data reinforce each other. It’s not a question of data correlations versus theory. The use of data for correlations allows one to test theories and refine them.”

-- John Seely Brown, Independent Co-Chair of Deloitte Center for the Edge, believes that we may need to devise new methods of theory formation: “One of the big problems [with Big Data] is how to determine if something is an outlier or not,” and therefore can be disregarded. “In some ways, the more data you have, the more basis you have for deciding that something is an outlier. You have more confidence in deciding what to knock out of the data set—at least, under the Bayesian and correlational-type theories of the moment.”
But this sort of theory-formation is fairly crude in light of the keen
and subtle insights that might be gleaned from Big Data, said Brown:
“Big Data suddenly changes the whole game of how you look at the
ethereal odd data sets.”  Instead of identifying outliers and “cleaning” datasets, theory formation using Big Data allows you to “craft an ontology and subject it to tests to see what its predictive value is.” “...The more data there is, the better my chances of finding the ‘generators’ for a new theory.”

**Visualization as a Sense-Making Tool**
-- Stensrud agreed that visualization of Big Data gives you a way “to find things that you had no theory about and no statistical models to identify, but with visualization it jumps right out at you and says, ‘This is bizarre.’ ”
-- Or as Lise Getoor, Associate Professor in the Department of Computer Science at the University of Maryland, articulated, visualizations allows researchers to “ ‘explore the space of models’ in more expansive ways.  They can combine large data sets with statistical analysis and new types of computational resources to use various form functions in a systematic way and explore a wider space.”
-- Kim Taipale of the Center for Advanced Studies in Science and
Technology warned that visualization design choices drive results every bit as much as traditional “data-cleaning” choices.  Visualization techniques contain embedded judgments.
-- "“You need to make sure the pattern that you think is there, is actually there,” said Andersen.  “Otherwise, the problem gets worse the bigger your data is—and we don’t have any idea how to handle that in visualization because there is a very, very thin layer of truth on the data, because of tricks of the eye about whether what you see is actually there.  The only way that we can solve this problem right now is to protect ourselves with a model.”"

--If you’re trying to predict the weather in New England from looking at the weather patterns in California, you’ll have a problem.  That’s why you need the whole system.  You’re not going to need every molecule in that system; you might be able to deal with every weather station, or some degree of aggregation that’s going to make the analysis a lot easier. Hal Varian, Chief Economist at Google
-- Bill Stensrud took issue with this approach as a general rule:  “If you know what questions you’re asking of the data, you may be able to work with a 2 percent sample of the whole data set.  But if you don’t know what questions you’re asking, reducing it down to 2 percent means that you discard all the noise that could be important information.  What you really want to be doing is looking at the whole data set in ways that tell you things and answers questions that you’re not asking.”
-- Bill Stensrud agreed, “Everything I buy from Amazon is a present for somebody else, and so their recommendation engine is meaningless to me.  Some day they’ll figure that out.”

-- Correlations can be functional and useful in stimulating sales and
making money, but they can also be highly imperfect.

-- “Google doesn’t know how many Jeff Jonas’s there are,” said Jeff
Jonas of IBM Software Systems.  “If you can’t do correlations at atomic level construction [counting discrete identifiable units], you can’t really do any form of meaningful predictions because you can’t get trajectory or velocity [from the data].”

-- For Joi Ito, the Chief Executive Officer of Creative Commons, the
search for correlations is a trap to be avoided, at least in his capacity of a computer security expert and a venture capitalist.  Ito says he is “always looking for unpredictable things that you can use opportunistically.”  As a venture capitalist, he is looking for the “subversive outlier” whose ideas could have a big upside.  From a security perspective, Ito says he wants to be alert to the unexpected forms of intrusion and deceit, not to the ones whose correlations can be easily discovered using computers.

When you do that kind of analysis on, say, terrorist networks, you have to understand that Hezbollah is actively trying to continuously come up with patterns that they think you won’t predict.” “Remember,” said Ito, “the same technology that we’re using to analyze Big Data enables these other actors to become more actively random.  The people who are outliers, who used to sort of behave randomly, now have access to the same tools as the rest of us and are looking at the same data.

“Big Data is about exactly right now , with no historical context that is predictive,” said Ito.  “It’s predictive of a linear thing—but you can use data collection to discover non-linearity as well. ... It’s important not to be obsessed with the old models that come from the old data. It’s more important to be ignorant enough to come up with a new model of the future.”

-- Many innovative uses of Big Data could be called “now-casting,”
said Varian. This term refers to the use of real-time data to describe contemporaneous activities before official data sources are available. “We’ve got a real-time variable, Google search queries, which are pretty much continuous,” said Varian.  “Even if all you’ve got is a contemporaneous correlation, you’ve still got a six-week lead on the reported values” for certain types of data.
--  “To make money, you’ve got to predict two things—what’s going to happen and what people think is going to happen.
-- According to Jacques Bughin of McKinsey and Company, the real
insight is that Big Data has the potential to discover new laws of macrobehaviors totally overlooked with the paucity of data of the past. Although social influence is and remains large, a new type of market power behavior has emerged, one that is not necessarily firm driven, but consumer driven.

-- “Financial markets are at least as complicated and important as the weather, but we don’t have the equivalent of a national weather service or a national hurricane center, for the financial markets.” John Liechty

-- "Large databases also open up all sorts of new business opportunities. **“Now-casting”** is helping companies understand the real-time dynamics of certain areas of life—from the diffusion of diseases to consumer purchases to night-life activity—which will have many long-term reverberations on markets. New types of **data-intermediaries** are also likely to arise to help people make sense of an otherwise-bewildering flood of information.  Indeed, data-intermediaries and interpreters could represent a burgeoning segment of the information technology sector in the years ahead."

eagle2006reality
----------------
-- ^ A recent M.I.T. study found that geo-location data patterns can successfully predict people’s future locations and social interactions.
-- also with dataset

pietsch2015aspects
-------------------
With respect to the novelty of data-intensive science, I draw an analogy to exploratory as opposed to theory-directed experimentation.

siegfried2013big
-----------------
"...sometimes Big Data In means Bad Data Out. Wringing intelligent insights from Big Data poses formidable challenges for computer science, statistical inference methods and even the scientific method itself."

"Computer scientists, of course, have made the accumulation of all this big data possible by developing exceptional computing power and information storage technologies. But collecting data and storing information is not the same as understanding it. Figuring out what Big Data means isn’t the same as interpreting little data, just as understanding flocking behavior in birds doesn’t explain the squawks of a lone seagull."

"Many statistical procedures either have unknown runtimes or runtimes that render the procedure unusable on large-scale data,” writes Michael Jordan of the University of California, Berkeley. “Faced with this situation, gatherers of large-scale data are often forced to turn to ad hoc procedures that … may have poor or even disastrous statistical properties." -- <https://arxiv.org/abs/1309.7804>

bd are “high dimensional.” More dimensions raises the risk of finding spurious correlation.


"In particular, we emphasis on the viability of the sparsest solution in high-confidence set and point out that exogeneous assumptions in most statistical methods for Big Data can not be validated due to incidental endogeneity. They can lead to wrong statistical inferences and consequently wrong scientific conclusions."
"Besides that, Big Data often is acquired by combining information from many sources, at different times, using a variety of technologies or methodologies, Fan and colleagues point out. “This creates issues of heterogeneity, experimental variations, and statistical biases, and requires us to develop more adaptive and robust procedures,” they write. “To handle the challenges of Big Data, we need new statistical thinking and computational methods.”
<https://arxiv.org/abs/1308.1479> @fan2014challenges

"In fact, the arrival of Big Data should compel scientists to cope with the fact that nature itself is the ultimate Big Data database. Old style science coped with nature’s complexities by seeking the underlying simplicities in the sparse data acquired by experiments. But Big Data forces scientists to confront the entire repertoire of nature’s nuances and all their complexities."

... science cannot rely on the strictly empirical approach to answer questions about complex systems. There are too many possible factors influencing the system and too many possible responses that the system might make in any given set of circumstances. To use Big Data effectively, science might just have to learn to subordinate experiment to theory.

west2013big
------------

Our traditional approaches to these problems (big global issues) are often qualitative and disjointed and lead to unintended consequences. To bring scientific rigor to the challenges of our time, we need to develop a deeper understanding of complexity itself.

What makes a “complex system” so vexing is that its collective characteristics cannot easily be predicted from underlying components: the whole is greater than, and often significantly different from, the sum of its parts. A city is much more than its buildings and people. Our bodies are more than the totality of our cells. This quality, is called **emergent behavior**

The trouble is, we don't have a unified, conceptual framework for addressing questions of complexity. We don't know what kind of data we need, nor how much, or what critical questions we should be asking. “Big data” without a “big theory” to go with it loses much of its potency and usefulness, potentially generating new unintended consequences.

What are the underlying principles that transcend the extraordinary diversity and historical contingency and interconnectivity of financial markets, populations, ecosystems, war and conflict, pandemics and cancer? An overarching predictive, mathematical framework for complex systems would, in principle, incorporate the dynamics and organization of any complex system in a quantitative, computable framework.


also <http://www.methodspace.com/rob-kitchin-big-data-complement-not-replace-small-data-not-replace/>
----------------------------------------

-- first question:
-- census data vs social media
-- remote sensing vs various devices
-- key attribute *velocity*

uprichard2013focus
---------------------

"After all, most big data is and will continue to be social data. "

"At the risk of sounding a bit melodramatic, the big data hype is generating, for want of a better term, a methodological genocide. To my mind, it even has a flavour of being a disciplinary genocide."

"If we take C. Wright Mills’ quest for a ‘sociological imagination’ seriously, then ideally we need to also turn to big data to help us think differently, to see differently and re-enact the world differently. So much social theory has gone into arguing and discussing the s e very issue s and we cannot afford to l et big data run away without good social theories about what to do with the masses of data we are producing. Bourdieu (1990:64) warned us about the limits of ch ange when we become complicit to our ‘structuring structures’ that tend to make us ‘cut our coats according to our cloth’, and so we become ‘the accomplices of the processes that tend to make the probable a reality’. If we are creating a mess by generating so many haystacks of big data that we are losing all the needles, then we need to figure out a different kind of way of doing things, as we cannot sew new cloth without any needles. Whatever else we make of the ‘big data’ hype, it cannot and must not be the path we take to answer all our big global problems. On the contrary, it is great for small questions, but may n ot so good for big social questions. Social scientists need to find a way not to be complicit in the new wave of Methodenstreit that is intrinsic to what big data brings."


heffernan <http://www.cbsnews.com/news/big-data-big-risk/>
----------------------------------------------------------

‘Big data will never give you big ideas... Big data doesn’t facilitate big leaps of the imagination. It will never conjure up a PC revolution or any kind of paradigm shift. And while it might tell you what to aim for, it can’t tell you how to get there.’

tukey1997more
---------------

"no data set is large enough to provide complete information about how it should be analysed!"


some notes from Massey -- For Space
-------------------------------------

3 common ways of thinking about space:
-- space as a surface we are placed on
-- turning space into time (space as slice through time)
-- sharp separation of local place and the world out there

^ those are failures of spatial immaginations

Her propositions
-- space is a prodct of interrelations, constituted through interactions
-- space is a condition for multiplicity, in space different trajectories coexist, space is predicated upon the existence of plurality
-- space is always under construction, never finished, never closed, it is a "simultaneity of stories so far"

Representation fixes (and deadens) time but also space, space is same as time unrepresentable in complete mimetic sense. Representation can be thought of as *spatialization* of the represented but can not be mistaken for the space itself (So beware of "spatial" metaphores that are used so automatically in present language).

Fabian 83 -- time and the other: "...a taxonomic space, indeed a map".

...not only might we productively conceptualise space in terms of relations but also relations can only be fully recognised by thinking fully spatially. In order for there to be relations there must of necessity be spacing.

-- space as a sphere of coexisitng multiplicity
-- ...maps (current Western-type maps) give the impression that space is a surface -- that it is the sphere of completed horizontality.
-- space -- not a discrete multiplicity of iner thins, rather heterogenity of practices and processes
-- everything in space has it's own temporality (history) (btw would be nice to be able to display it on a map, sort of a thermal image showing age instead of temperature). History had is own geographies too, so time and space are not contradictory, they are interrelated.
-- "everyting is connected to everything else" -- good reminder that our actions have wider implications, but unhelpful if it leads to a conception of already consituted holism (similar to what map shows). Space is potential links and connections yet to be made, loose ends and ongoing stories. "Loose ends and ongoing stories are real challenges to cartography".
-- Huggan 89 Decolonising the map... -- maps "exemplary structuralist activity"
-- maps are conceptual and a-temporal -- but ironically, given that these are maps, they are not spatial --structures

-- Rabasa 93 -- Inventing america..: The Atlas thus consitutes a world where all possible surprises have been precodified. ...As such tha Atlas is palimpsest (a complex palimpsest of allegories). -- about Mercator Atlas
-- on map ...We don not feel the disruptions of space, the coming upon diffence. On the road map you won't drive of the edge of your known world. In space, as I want to imagine it, you just might.


lewitt2016more
--------------
some notes:
-- ...placing a thermal camera on a shop floor to create  a real-time record of activity that can be monitored. ...the data retrieved through the process of monitoring and localizing  regularities and tendencies within the gradient of thermal activity on the shop floor, emerges by virtue of making that data into grid, and then providing planning and efficiency recommendations based on that grid.
-- he cites  @siegert2015cutural -- grids are able to merge representation and control into 3 features:
1. -- bodies located in 3D space are located, given address and projected to 2D plane
2. -- addres becomes freighted data -- allowing data retrieval based on location
3. -- this operationalizes "deixes" -- binding phenomena to contextual matrix, subjecting whatever the grid locates to processes of placement (or enframing -- Gestell), storage and retrieval.

-- Taylorization -- strategy of efficient motion planning
-- Grid prescribes a place, while a grade describes a performance.
-- The conditions of circulation seem to have superseded those of the specificity of any site whatsoever. -- term "weak locality" replacing the concept of site.

boyd2011six
------------
Six provocations for big data

1. Automating Research Changes the Definition of Knowledge.
2. Claims to Objectivity and Accuracy are Misleading
3. Bigger Data are Not Always Better Data
4. Not All Data Are Equivalent
5. Just Because it is Accessible Doesn’t Make it Ethical
6. Limited Access to Big Data Creates New Digital Divides


ruzzenenti2009complexity
--------------------------
complexity and spatial systems?
http://uconn.academia.edu/RodSwenson
^ something useful towards visualisation there?


mayer2013big
-------------

NOW
-- "Data was no longer regarded as static or stale, whose usefulness was finished once he purpose for which it was collected was achieved [...]. Rather, data became a raw material of business, a vital econimic input, used to create a new form of econimic value."

-- "Every single dataset is likely to have some intristic, hidden, not yet unearthed value..."

-- Big data is not about trying to "teach" computer to "think" like humans. Instead it's about applying math to huge quantities of data in order to infer probabilities.

-- Analogy: many images fast make a movie. "A movie is fundametally different from a frozen photograph. It's the same with big data: by changing the amount, we change the essence."

-- "Sometimes the constraints that we lice with, and persume, are the same for everything, are really only functions of the scale in which we operate." -- e.g. the main operting law of physics for us is gravity, for insects surface tension. Or nanotechonlogy -- at scale small enoug phyiscal properties change. Same with information, with data big enough there are mindset changes comming:

Forseen changes:
-- no more need for sampling
-- loosen exactitude -- "with less error from sampling we can accept more measurement error."
-- move away from search for causality -- "big data is about *what* not *why*." Multi factor correlation with large data enables us to decide even if we do not know the mechanism behind the relationsip.


MORE (n=all is better than samples)

... using n=all means we can drill down deep into data; sample can't do that nearly as well. ...(witht sampling), you lose the detailsm the granularity, the ability to look closer at certian subgroups. (me: this can be done with visualisation methods)

...The outliers are the mist interesting information, and you can only identify them in comparison to the mass of normal transactions. It is a big-data problem (example of finding suspicious credit card transactions)

...(in social sciences) big-data anaylsis replaces the highly skilled survey specialists of the past. The social science disciplines largely relied on sampling studies and questionnaries. But when the data is collected passively while people do what they normally do anyway, the old biases associated with sampling and questionnaries disappear.

@onnela2007structure -- observing social network through mobile operator data

MESSY

However, in many new situations that are cropping up today, allowing for imprecision -for messiness - may be a positive feature, not a shortcoming. It is a tradeoff. In return for relaxing the standards of allowable errors, one can get ahold of much more data. It isn’t just that “more trumps some,” but that, in fact, sometimes “more trumps better.”

...we can accept some messiness in return for scale.  Of course the data can’t be completely incorrect, but we’re willing to sacrifice a bit of accuracy in return for knowing the general trend. Big data transforms figures into something more probabilistic than precise.

...clean taxonomies are being replaced by mechanisms that are messier but also eminently more flexible and adaptable to aworld that evolves and chages. (example flickr tags)

-- bilion prices project <http://bpp.mit.edu/>
-- If You Have Too Much Data, then "Good Enough" Is Good Enough @helland2011if

CORRELATION

... experts used hypotheses driven by theories. Based on such hypotheses, they collected data and used correlation analysis to verify whether the proxies were suitable. If they weren’t, then the researchers often tried again, stubbornly, in case the data had been collected wrongly, before finally conceding that the hypothesis they had started with, or even the theory it was based on, was flawed and required amendment. Knowledge progressed through this hypothesis-driven trial and error. And it did so slowly, as our individual and collective biases clouded what hypotheses we developed, how we applied them, and thus what proxies we picked.

In the big-data age, it is no longer efficient to make decisions about what variables to examine by relying on hypotheses algae. The datasets are far too big and the area under consideration is probably far too complex. Fortunately, many of the limitations that forced us into a hypothesis-driven approach no longer exist to the same extent. We now have so much data available and so much computing power that we don’t have to laboriously pick one proxy or a small handful of them and examine them one by one. Sophisticated computational analysis can now identify the optimal proxy -as it did for Google Flu Trends, after plowing through almost half a billion mathematical models.

No longer do we necessarily require a valid substantive hypothesis about a phenomenon to begin to understand our world.

In a small-data world, because so little data tended to be available, both causal investigations and correlation analysis began with a hypothesis, which was then tested to be either falsified or verified. But because both methods required a hypothesis to start with, both were equally susceptible to prejudice and erroneous intuition.

There is another difference, which is just starting to gain inrportance. Before big data, partly because of inadequate computing power, most correlational analysis using large data sets was limited to looking for linear relationships. In reality, of course, many relationships are far more complex. With more sophisticated analyses, we can identify non-linear relationships among data.

As one example, for many years economists and political science we never saw before. We will grasp complex technical and social dynamics that have long escaped our comprehension despite our best efforts. But most important, these non-causal analyses will aid our understanding of the world by primarily asking what rather than why.

Big data will not spell the "end of theory", but will fundametally transform the way we make sense of the world.

DATAFICATION

-- unearthing data from material no one thought had any value.
-- using data differently than was the original intention for capturing them
-- once location is datafied, new uses crop up
-- Seeing the world as information, as oceans of data that can be ex plored at ever greater breadth and depth, offers us a perspective on reality that we did not have before. It is a mental outlook that may penetrate all areas of life. Today, we are a numerate society because we presume that the world is understandable with numbers and math. And we take for granted that knowledge can be transmitted across time and space because the idea of the written word is so ingrained. Tomorrow, subsequent generations may have **big-data consciousness** -the presumption that there is a quantitative component to all that we do, and that data is indispensable for society to learn from.

-- study on slums eagle2010big

VALUE

-- how to value big data? example of facebook stocks (way smaller than expected, because data not taken into account and "the company is nothing but data")

IMPLICATIONS

-- three types of bd companies: that have data, that have skills, and that have ideas -- over time the importance of data holder will increase
-- subject area experts will not die out, but will have to accept results of bd correlations
-- bd breadth and sme depth
-- shift to data-driven decisions  (now many people base their decisions on a combination of facts, reflections and guesswork)
-- individual may gain more power over their data and sell them (maybe through som intermediaries e.g. <https://mydex.org/>, <https://idcubed.org/>)

RISKS

-- penalties based on propensities (like minority report)
-- strategies for privacy doesnt work in bd world:
-- individual notice and consent -- done only for the original use of data, not unforseen secondary uses
-- anonymizations -- bd facilitates re-identification from other sources (example of "anonymized" netflix user data -- users reidentified e.g. by comparing with imdb scores)
-- big data predictions are hard to challenge -- no causation, though allow for more individual based predictions, rather than group victimization

CONTROL

-- responding to risks, more topics for lawyers -- data holder is responsible for uses of data no matter the consent

NEXT

-- future: bayesian stuff -- results from previous observations influnece the probability of an event in a next observation

