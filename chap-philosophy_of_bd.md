# Philosophy of Big Data

@bollier2010promise
-------------------
-- some citations about usefulness of therory -- might be useful
-- Using correlations as the basis for forecasts can be slippery for other reasons.  Once people know there is an automated system in place, they may deliberately try to game it.  Or they may unwittingly alter their behavior.

**how to craft theories in age of big data?**
-- Patrick W. Gross, Chairman of the Lovell Group, challenged the either/or proposition that either scientific models or data correlations will drive future knowledge. “In practice, the theory and the data reinforce each other. It’s not a question of data correlations versus theory. The use of data for correlations allows one to test theories and refine them.”

-- John Seely Brown, Independent Co-Chair of Deloitte Center for the Edge, believes that we may need to devise new methods of theory formation: “One of the big problems [with Big Data] is how to determine if something is an outlier or not,” and therefore can be disregarded. “In some ways, the more data you have, the more basis you have for deciding that something is an outlier. You have more confidence in deciding what to knock out of the data set—at least, under the Bayesian and correlational-type theories of the moment.”
But this sort of theory-formation is fairly crude in light of the keen
and subtle insights that might be gleaned from Big Data, said Brown:
“Big Data suddenly changes the whole game of how you look at the
ethereal odd data sets.”  Instead of identifying outliers and “cleaning” datasets, theory formation using Big Data allows you to “craft an ontology and subject it to tests to see what its predictive value is.” “...The more data there is, the better my chances of finding the ‘generators’ for a new theory.”

**Visualization as a Sense-Making Tool**
-- Stensrud agreed that visualization of Big Data gives you a way “to find things that you had no theory about and no statistical models to identify, but with visualization it jumps right out at you and says, ‘This is bizarre.’ ”
-- Or as Lise Getoor, Associate Professor in the Department of Computer Science at the University of Maryland, articulated, visualizations allows researchers to “ ‘explore the space of models’ in more expansive ways.  They can combine large data sets with statistical analysis and new types of computational resources to use various form functions in a systematic way and explore a wider space.”
-- Kim Taipale of the Center for Advanced Studies in Science and
Technology warned that visualization design choices drive results every bit as much as traditional “data-cleaning” choices.  Visualization techniques contain embedded judgments.
-- "“You need to make sure the pattern that you think is there, is actually there,” said Andersen.  “Otherwise, the problem gets worse the bigger your data is—and we don’t have any idea how to handle that in visualization because there is a very, very thin layer of truth on the data, because of tricks of the eye about whether what you see is actually there.  The only way that we can solve this problem right now is to protect ourselves with a model.”"

--If you’re trying to predict the weather in New England from looking at the weather patterns in California, you’ll have a problem.  That’s why you need the whole system.  You’re not going to need every molecule in that system; you might be able to deal with every weather station, or some degree of aggregation that’s going to make the analysis a lot easier. Hal Varian, Chief Economist at Google
-- Bill Stensrud took issue with this approach as a general rule:  “If you know what questions you’re asking of the data, you may be able to work with a 2 percent sample of the whole data set.  But if you don’t know what questions you’re asking, reducing it down to 2 percent means that you discard all the noise that could be important information.  What you really want to be doing is looking at the whole data set in ways that tell you things and answers questions that you’re not asking.”
-- Bill Stensrud agreed, “Everything I buy from Amazon is a present for somebody else, and so their recommendation engine is meaningless to me.  Some day they’ll figure that out.”

-- Correlations can be functional and useful in stimulating sales and
making money, but they can also be highly imperfect.

-- “Google doesn’t know how many Jeff Jonas’s there are,” said Jeff
Jonas of IBM Software Systems.  “If you can’t do correlations at atomic level construction [counting discrete identifiable units], you can’t really do any form of meaningful predictions because you can’t get trajectory or velocity [from the data].”

-- For Joi Ito, the Chief Executive Officer of Creative Commons, the
search for correlations is a trap to be avoided, at least in his capacity of a computer security expert and a venture capitalist.  Ito says he is “always looking for unpredictable things that you can use opportunistically.”  As a venture capitalist, he is looking for the “subversive outlier” whose ideas could have a big upside.  From a security perspective, Ito says he wants to be alert to the unexpected forms of intrusion and deceit, not to the ones whose correlations can be easily discovered using computers.

When you do that kind of analysis on, say, terrorist networks, you have to understand that Hezbollah is actively trying to continuously come up with patterns that they think you won’t predict.” “Remember,” said Ito, “the same technology that we’re using to analyze Big Data enables these other actors to become more actively random.  The people who are outliers, who used to sort of behave randomly, now have access to the same tools as the rest of us and are looking at the same data.

“Big Data is about exactly right now , with no historical context that is predictive,” said Ito.  “It’s predictive of a linear thing—but you can use data collection to discover non-linearity as well. ... It’s important not to be obsessed with the old models that come from the old data. It’s more important to be ignorant enough to come up with a new model of the future.”

-- Many innovative uses of Big Data could be called “now-casting,”
said Varian. This term refers to the use of real-time data to describe contemporaneous activities before official data sources are available. “We’ve got a real-time variable, Google search queries, which are pretty much continuous,” said Varian.  “Even if all you’ve got is a contemporaneous correlation, you’ve still got a six-week lead on the reported values” for certain types of data.
--  “To make money, you’ve got to predict two things—what’s going to happen and what people think is going to happen.
-- According to Jacques Bughin of McKinsey and Company, the real
insight is that Big Data has the potential to discover new laws of macrobehaviors totally overlooked with the paucity of data of the past. Although social influence is and remains large, a new type of market power behavior has emerged, one that is not necessarily firm driven, but consumer driven.

-- “Financial markets are at least as complicated and important as the weather, but we don’t have the equivalent of a national weather service or a national hurricane center, for the financial markets.” John Liechty

-- "Large databases also open up all sorts of new business opportunities. **“Now-casting”** is helping companies understand the real-time dynamics of certain areas of life—from the diffusion of diseases to consumer purchases to night-life activity—which will have many long-term reverberations on markets. New types of **data-intermediaries** are also likely to arise to help people make sense of an otherwise-bewildering flood of information.  Indeed, data-intermediaries and interpreters could represent a burgeoning segment of the information technology sector in the years ahead."

eagle2006reality
----------------
-- ^ A recent M.I.T. study found that geo-location data patterns can successfully predict people’s future locations and social interactions.
-- also with dataset

pietsch2015aspects
-------------------
With respect to the novelty of data-intensive science, I draw an analogy to exploratory as opposed to theory-directed experimentation.

siegfried2013big
-----------------
"...sometimes Big Data In means Bad Data Out. Wringing intelligent insights from Big Data poses formidable challenges for computer science, statistical inference methods and even the scientific method itself."

"Computer scientists, of course, have made the accumulation of all this big data possible by developing exceptional computing power and information storage technologies. But collecting data and storing information is not the same as understanding it. Figuring out what Big Data means isn’t the same as interpreting little data, just as understanding flocking behavior in birds doesn’t explain the squawks of a lone seagull."

"Many statistical procedures either have unknown runtimes or runtimes that render the procedure unusable on large-scale data,” writes Michael Jordan of the University of California, Berkeley. “Faced with this situation, gatherers of large-scale data are often forced to turn to ad hoc procedures that … may have poor or even disastrous statistical properties." -- <https://arxiv.org/abs/1309.7804>

bd are “high dimensional.” More dimensions raises the risk of finding spurious correlation.


"In particular, we emphasis on the viability of the sparsest solution in high-confidence set and point out that exogeneous assumptions in most statistical methods for Big Data can not be validated due to incidental endogeneity. They can lead to wrong statistical inferences and consequently wrong scientific conclusions."
"Besides that, Big Data often is acquired by combining information from many sources, at different times, using a variety of technologies or methodologies, Fan and colleagues point out. “This creates issues of heterogeneity, experimental variations, and statistical biases, and requires us to develop more adaptive and robust procedures,” they write. “To handle the challenges of Big Data, we need new statistical thinking and computational methods.”
<https://arxiv.org/abs/1308.1479> @fan2014challenges

"In fact, the arrival of Big Data should compel scientists to cope with the fact that nature itself is the ultimate Big Data database. Old style science coped with nature’s complexities by seeking the underlying simplicities in the sparse data acquired by experiments. But Big Data forces scientists to confront the entire repertoire of nature’s nuances and all their complexities."

... science cannot rely on the strictly empirical approach to answer questions about complex systems. There are too many possible factors influencing the system and too many possible responses that the system might make in any given set of circumstances. To use Big Data effectively, science might just have to learn to subordinate experiment to theory.






some notes from Massey -- For Space
-------------------------------------

3 common ways of thinking about space:
-- space as a surface we are placed on
-- turning space into time (space as slice through time)
-- sharp separation of local place and the world out there

^ those are failures of spatial immaginations

Her propositions
-- space is a prodct of interrelations, constituted through interactions
-- space is a condition for multiplicity, in space different trajectories coexist, space is predicated upon the existence of plurality
-- space is always under construction, never finished, never closed, it is a "simultaneity of stories so far"

Representation fixes (and deadens) time but also space, space is same as time unrepresentable in complete mimetic sense. Representation can be thought of as *spatialization* of the represented but can not be mistaken for the space itself (So beware of "spatial" metaphores that are used so automatically in present language).

Fabian 83 -- time and the other: "...a taxonomic space, indeed a map".

...not only might we productively conceptualise space in terms of relations but also relations can only be fully recognised by thinking fully spatially. In order for there to be relations there must of necessity be spacing.

-- space as a sphere of coexisitng multiplicity
-- ...maps (current Western-type maps) give the impression that space is a surface -- that it is the sphere of completed horizontality.
-- space -- not a discrete multiplicity of iner thins, rather heterogenity of practices and processes
-- everything in space has it's own temporality (history) (btw would be nice to be able to display it on a map, sort of a thermal image showing age instead of temperature). History had is own geographies too, so time and space are not contradictory, they are interrelated.
-- "everyting is connected to everything else" -- good reminder that our actions have wider implications, but unhelpful if it leads to a conception of already consituted holism (similar to what map shows). Space is potential links and connections yet to be made, loose ends and ongoing stories. "Loose ends and ongoing stories are real challenges to cartography".
-- Huggan 89 Decolonising the map... -- maps "exemplary structuralist activity"
-- maps are conceptual and a-temporal -- but ironically, given that these are maps, they are not spatial --structures

-- Rabasa 93 -- Inventing america..: The Atlas thus consitutes a world where all possible surprises have been precodified. ...As such tha Atlas is palimpsest (a complex palimpsest of allegories). -- about Mercator Atlas
-- on map ...We don not feel the disruptions of space, the coming upon diffence. On the road map you won't drive of the edge of your known world. In space, as I want to imagine it, you just might.


mayer2013big
-------------

NOW

-- Analogy: many images fast make a movie. "A movie is fundametally different from a frozen photograph. It's the same with big data: by changing the amount, we change the essence."

-- "Sometimes the constraints that we lice with, and persume, are the same for everything, are really only functions of the scale in which we operate." -- e.g. the main operting law of physics for us is gravity, for insects surface tension. Or nanotechonlogy -- at scale small enoug phyiscal properties change.
