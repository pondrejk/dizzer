# 5 Case study: Lockdown traffic

In this case study we explore the possibilities and limitations of vector tiles to accommodate the temporal density of a data set originally published as a live stream. For this purpose we chose to visualize changes in traffic speeds in the city of Brno between the 16th of March to the 10th of May 2020^[The author would like to thank Mapbox, Inc. for providing the traffic data sample for the purpose of this case study.]. The time range coincidentally matches with the first period of government restrictions in Czech Republic to prevent the spread of the COVID-19 pandemic. The spatial and temporal detail of the data set posed a challenge both in terms of data processing as well as in terms of designing the interactive cartographic visualization^[Live demo of the application is accessible at <https://pondrejk.eu/traffic>, screenshots of the interface can be found in Appendix C.].

## 5.1 Data sources and transformations

The source raw data were formatted as (compressed) CSV files containing estimate traffic speeds for specific road segments, at a specific time, based on historical observations.  In terms of temporal coverage, one file contained the expected traffic speeds spanning across one week^[The official description of the data source can be found at <https://docs.mapbox.com/traffic-data/overview/data/>]. Spatially, one file covered the area of a zoom level 6 tile, which meant that the data files for our problem area also covered a significant part of the Czech Republic (Figure 36). 

![**Fig 36.** Czech Republic is covered by four tiles at zoom level six. Our area of interest, the Brno municipal region fits into tile 120212 (screenshot taken from <https://labs.mapbox.com/what-the-tile/>).](imgs/img-quadkeys.png)

As for the original CSV structure, one line in the file corresponded with one road segment. Each segment was identified by a pair of *Open Street Map node IDs* representing the start node and the end node of a segment. Notice that the node ordering also determined the direction of the recorded traffic. Owing to that, bidirectional routes were recorded twice in the file — one row for direction from node A to  B and another row for the opposite direction. 

Besides the two columns with node identifiers, each row contained 2016 more columns with speed estimates in 5 minute intervals per each segment (7 days × 24 hours × 12 five-minute periods in hour). An example of how a row could look like: *113054533,113096757,54,54,...57*, where the first two digits are node identifiers followed by an array of traffic speeds. All speeds were recorded in kilometers per hour. The first record represents Sunday 00:00 AM of the given week in the time zone of the mapped area. The records continue in 5 minute increments until the concluding one marking the end of the week.

This gives us an idea of the data volume that needed to be processed. One file with week-long data per zoom level 6 tile contained approximately 1 086 958 rows representing road segments (the line count could differ across the files because segments lacking sufficient data for a high confidence estimate were omitted). Each row contained 2018 records (speeds + identifiers) which exceeds the limit for the maximum column count per table in a PostgreSQL database (which is 1600)^[Just for completeness, the column limit can be extended, but this requires re-compiling the database from the source code. See <https://www.postgresql.org/docs/current/limits.html> for the overview of PostgreSQL limits]. Data was provided for eight weeks, so there were eight files of these proportions to be processed.

A number of tasks were completed in the initial phase of data processing. As the OSM node IDs as such do not contain spatial information, we needed to first obtain the actual coordinates for each node. This was done in the following steps. First, to minimize redundant API calls, we extracted the unique node IDs form the first two columns of our data files. As we have seen earlier, the node IDs can appear several times as route identifiers, either in bidirectional segments or in crossroads.^[The python script based on the *numpy* library that performed the unique node extraction can be found at <https://github.com/pondrejk/dizzer/blob/master/misc/scripts/01-get_unique_nodes.py>] For each of the unique nodes, we obtained spatial coordinates by querying the Open Street Map API.^[The script to do that using the *osm* Python library is available at <https://github.com/pondrejk/dizzer/blob/master/misc/scripts/02-get_node_coordinates.py>]

With spatially defined unique nodes it was possible to filter out the subset of the nodes that belonged to the Brno municipal area. The most straightforward way to do that is to load the nodes into QGIS to perform *select by location* against the polygon of the city area (with five kilometer buffer to provide some context of immediate surroundings). Armed with a collection of Brno nodes (the count was 131 257), we returned to the original traffic speed CSVs to extract the city's road segments, with speed attributes included. The challenge was in searching for 131 257 nodes in the superset of 1 086 958 lines and then extracting the matching lines in the full length of 2018 attributes.^[The script to perform this action using the *dask* Python library is available at <https://github.com/pondrejk/dizzer/blob/master/misc/scripts/03-select_segments.py>].

Such task is reminiscent of situations described in the *Small big data manifesto* [@voss2017small] — even though big data are mainly associated with large data center infrastructure, individuals increasingly come across situations when they need to process large data sets only with a single machine at their hands. Setting up a cluster of machines is not viable for many applications be it for lacking time, expertise or finances. So for one-time processing of data that does not fit into memory, we are left with a range of simple but often efficient computing tools and approaches [@turner2020process]. One of them is reading input data in chunks that can fit to memory, applying a processing function to them and then using a reducer function to combine the processed chunks into the final result. This way the memory size limitation is bypassed, but the computation time of the processing function can still be a bottleneck. Multi-threaded execution can ease the problem by running the function on several CPU cores in parallel. The size of chucks and the number of threads needs to be fine tuned to fit the capabilities of given hardware, but in general these techniques can significantly reduce the processing time even on modest machines. Cycling back to our speed files, a simple script combining chunking and parallelization (using the Dask Python library) was able to complete the extraction of Brno segments from one week file in 3 min 32.8s (on Intel i7 8 cores, 30 GiB RAM).

The extraction process yielded eight CSV files in the original structure holding the estimated speeds for road segments within Brno. These weekly files where split into smaller chunks representing daily speeds to avoid hitting the database column length limit^[Using this Python script <https://github.com/pondrejk/dizzer/blob/master/misc/scripts/04-split_by_day.py>]. The resulting set of 56 files, each with 288 columns of speed data, was loaded to the PostgreSQL database. 

At this point, the tables of Brno node pairs and node coordinates were also imported in order to create a line segment layer from the point coordinates using the PostGIS plugin ^[Example queries for actions described in this paragraph can be found at <https://github.com/pondrejk/dizzer/blob/master/misc/queries/>]. From now on, the daily speed tables could be joined with the table of line segments to create spatial layers. During this process various visualization experiments were done using QGIS connected to the database. In light of these experiments, we decided to reduce the temporal granularity of the speed layers from 5 minute intervals to one hour averages. This significantly reduced the storage overhead in generated vector tiles while maintaining sufficient information density for visualization purposes. 

A database loaded with road spatial layers with associated hourly speed attributes provides a solid starting point from which many avenues could be taken, either in analytical or visualization direction. We decided to explore the vector tile format and its aptness for powering interactive cartographic visualizations. Therefore we created the necessary amount of vector tiles from GeoJSON database exports using the tippecanoe command line tool. The batch of resulting *.mbtile* files was then uploaded to the Mapbox server via API^[The batch upload script is available at <https://github.com/pondrejk/dizzer/blob/master/misc/scripts/05-mapbox_upload.py>].


## 5.2 Application architecture

The building blocks of the application are basically the same as with the case study described in the previous chapter. Even though the PostgreSQL database played a vital role in the data preparation phase, the final application does not use it for back-end data storage. Instead, the vector tiles are loaded from Mapbox tile server. The front-end interface is built using the React library with Redux for state management, mapbox-gl is used as a rendering engine on the client.


## 5.3 Cartographic decisions

The main requirement for the map was an ability to display the whole data set at the zoom level 10 so that the municipal traffic network can be observed as a whole. The Mapbox tile server enforces a size limit of 500 KB per tile for the uploaded vector tiles. While this is a reasonable limitation to ensure fast rendering, it is hard to adhere to it with denser data sets especially at smaller scales where individual tiles cover larger area. One way to work around this is by limiting the number of features displayed across scales (see Figure 37), which obviously has a downsize in loosing some resolution of the visualized data.

![**Fig. 37** Live Mapbox traffic data layer at three zoom levels (city of Brno, largest scale on the left). While reducing the number of displayed roads based on importance is a way to maintain fast tile rendering, in case of Brno it precludes observing the city's traffic network as a whole. Styling in Mapbox Studio by the author.](imgs/img-traffic-scaling.png)

Another way to grapple with this problem without abandoning the Mapbox infrastructure is to break up the tile layers into sub-layers with smaller attribute count. Using this approach we achieved full spatial resolution at the zoom level 10 — we halved the tile layers with daily speed coverage, so that each covered twelve hours. This got us below the tile size limit but also impacted the smoothness of the user experience on the client.

When rendering the layer with data driven style in mapbox-gl, any user-induced changes are rendered smoothly when the attribute change is within the same layer. Once a different layer needs to be loaded, there always is a visible glimpse between hiding the previously displayed layer and enabling the new one, which unfortunately cannot be treated by any ease-in effect. One way to work around this is keeping all the layers in the visible state and change their order dynamically so that the selected layer is on top — however our testing proved this approach inefficient for large number of layers. In our case it was two layers per day, so 2 x 7 (days) x 8 (weeks) = 112 layers. 

The color scale selected to visualize traffic speeds spans from 0 to 140 kilometers per hour in 10 kilometer intervals. The color selection was guided by the need for sufficient contrast on the dark background. The break between the two hues used in the color scale rests at 60 km which should ensure the variability of speeds is visible within the slower inner-city routes while the distinction from fast transit highways is apparent. 

The *offset* styling parameter allows to displace a line symbol from its spatial delineation by a certain distance to the side (relative to line direction). This parameter had to be applied so that the symbols for bidirectional routes are both visible — otherwise the lines would overlap as the defining nodes of these segments have the same coordinates, it is only the node ordering that differentiates them. Styling across the zoom range has been applied to both the line width and the line offset to secure a reasonable graphic fill across scales (Figure 38).

![**Fig. 38** The example of styling across the zoom range used in the application. The street line width is changing exponentially with scale (0.1px at zoom level 10, 2px at zoom level 14, 5px at zoom level 16). Screenshot from Mapbox Studio, a web-based tool to create and asses styles for vector tiles.](imgs/img-line-width.png)

There is a range of cartographic methods that would allow comparison between the state of the traffic network in two moments. A dual map view or a difference layer would both be viable, though we wanted to employ some WebGL specific features of that would not be readily available in SVG or Canvas environments. For that matter, we chose to apply the *fill-extrusion* parameter combined with tilted camera view. Fill-extrusion is a method intended for use with polygon layers mainly to create 3D building models. But it is well applicable to line layers as well. The *base height* fill-extrusion parameter allows to "lift" the shape off the ground, which enabled stacking multiple stripes on top of each other (Figure 39). Color coding of the fill-extrusion is still dynamically adjustable, which enables comparison of speed changes across weeks. This method certainly has its limitations: it is not well suited for global comparison, tilted camera view is necessary as well as user activity to pan and change the view angle. With bidirectional routes, each direction determines the color of one side of the 3D stripe, so the directions can't be viewed simultaneously.

![**Fig. 39** Screenshot from the application in the comparison mode. The fill-extrusion parameter is used to support comparison across weeks.](imgs/img-streets-comparison.png)


## 5.4 User interface design

The resulting application works in two modes, the default *single-layer mode* supports displaying the traffic situation in a specified date and time. The *comparison mode* then allows to view differences between selected weeks. The main points of interaction in the default mode are the controls for selecting hour, day and week. There is a certain redundancy in those controls: user can either click the *back* and *forward* arrow buttons (right to the map in the large screen view), which allow to "jump" back and forth in time in fixed intervals. This enables observing speed changes throughout the day (by moving in the hourly interval), comparing the situation in given time across days (daily interval) or seeing the difference between weeks (weekly interval). The currently selected date, time and week are displayed in the numerical form on the right pane and are also visualised using the table below the map where rows represent weeks and columns represent days. The selected day is highlighted by the table field color. The table not only shows the temporal extent of the data set and the current selection but also offers an alternative way to select the day and week by clicking the respective field. The slider below the map is coupled and aligned with the table. It provides a way to select an hour within the given week and also allows for smoother transition between map states.

In the comparison mode, which can be enabled by clicking the *compare* checkbox, some of the mentioned controls behave a bit differently. This mode allows for selecting two weeks which is facilitated by two drop-down menus. The hour and day selectors on the right pane remain active, whereas the week selector is disabled in favor of the drop-downs. The hour slider also remains active as it allows to change hour and day for both selected weeks simultaneously. It is also possible to change the observed day by clicking columns in the overview table (that now has got two fields highlighted to denote the selected weeks). 

We ensured responsiveness of the interface layout. The map field, the selection slider and the table are sized dynamically by the screen width. On small screens the right-side control pane is moved below the map field to leave sufficient screen width for the map. The legend is fixed to the right margin of the interface, which on the one hand places it out of the spotlight on large screens, but on the other hand it makes sure that the legend is placed next to the map field across all screen sizes.


## 5.5 Evaluation and possible extensions

During the preparation phase, the tile size limit of 500 KB appeared as an unforeseen driving factor that influenced our decision making both in data and visual space. While there are some alternative solutions like setting up a custom tile server, we chose to split vector tiles into chunks of the same spatial coverage but shorter time coverage to reduce the attribute count. This is a proven solution within the selected infrastructure, however more experimental approaches were tested over the course of the work. One of them is based on the fact that many road segments exhibited consequent runs of same speed values. The idea was to use a *run length encoding* algorithm to compress the attributes, which was successfully done on the database side^[Script using *pandas* and *sqlalchemy* Python libraries available at <https://github.com/pondrejk/dizzer/blob/master/misc/scripts/06-run_length_encode.py>] — the encoded values were stored in an array type column in PostgreSQL and then exported as vector tile layers. However, decoding the values on the client side showed to be beyond the scope of the mapbox-gl style definition language. There is also a question of the rendering performance as decoding the run length array and finding the right value for the selected time would have to be done for each displayed segment with every application state change.

Another solution would be to classify the streets in the data set, mainly to separate the segments with low speed variability that could be represented by a single value for a longer then hourly period. Overall, we identified three types of road segments in our problem area:

- Routes with low average speed and very low speed variability throughout the day. These are typically short segments with low traffic, cul-de-sacs leading to residential areas. 
- Routes with medium speeds and visible daily variability. These are mainly the inner-city veins
- Routes with high average speeds and small daily speed variability. These are transit highways with high allowed speeds. 

In this setting, it is hard to make any confident statements about the impact of the COVID-induced lockdown in the area. Only the second type of road segments seems to be sensitive to the impact of governmental restrictions. Also, traffic speed is only an indirect indicator of traffic volumes. Lowered number of vehicles during the lockdown period could have contributed to higher speeds in otherwise notoriously congested areas on the main city lines and near highway entrance ramps. Another impact could be in evening out the daily changes in traffic speeds and lowering the significance of peak hours.

There are of course many areas in which the application could be extended. For example, summary data on individual road segments could be displayed upon selection. Additional modes of comparison could be added as well as a capability to search and filter the mapped data e.g. by street name. A detailed overview of the speed changes across a user-selected route could be implemented, possibly with comparison of multiple itineraries. Reader may surely think of other extensions. To conclude, the application underlines the potential of the vector tile format combined with WebGl based rendering environment to handle both spatially and temporally dense data sets. Depicting the subtle changes in traffic speed patters lets us at the minimum appreciate the collective organism or the city. 


