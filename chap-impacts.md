Some Impacts of Data Abundance
-------------------------

*This chapter explores some impacts of big data deluge, tracks discussion and controversies from standpoints of science, business, and society. The possible role of (cartographic) visialisation in tackling these issues is also considered*

# Post-theory and hypothesis-less science?
# Correlation is enough?
# Ethic/Legal/... Remedies. Towards big data literacy (data-awareness)
# Data to Spatial to Visual (= cartography) for tackling complexity

## Science

-- data science, post theory

More on *value* -- the potential market value is potentially huge ("The potential value of Big Data is huge. Speaking about new Big Data initiatives in the US healthcare system last year, McKinsey estimated if these initiatives were rolled out system-wide, they “could account for $300 billion to $450 billion in reduced health-care spending, or 12 to 17 percent of the $2.6 trillion baseline in US health-care costs”. However, the cost of poor data is also huge- it’s estimated to cost US businesses $3.1 trillion a year. In essence, data on its own is virtually worthless. The value lies in rigorous analysis of accurate data, and the information and insights this provides." says @mcnulty2014understanding link on McKinsey <http://www.mckinsey.com/industries/healthcare-systems-and-services/our-insights/the-big-data-revolution-in-us-health-care>), but it "has the potential to cost unlimited amounts of money" @fischer2015why.


@anderson2008end: "where massive amounts of data and applied mathematics replace every other tool that might be brought to bear ... Who knows why people do what they do? The point is they do, and we can track it and measure it with unprecedented fidelity. With enough data, the numbers speak for themselves."


@wilson2017big: 
"In fact, the methods used to analyze large statistical data sets are themselves multiple, rather than singular. The type and context of the data govern the choices of the type of relevant analysis (for example, regression, significance testing, network analysis, frequency domain analysis, edge detection, decomposition, deconvolution, or machine learning using e.g., artificial neural nets). What is more, in traditional (regression) analyses, although algorithms can be written to extract patterns, the researcher or programmer constructing the algorithm must have imagined the pattern to be possible; and while advances in machine learning mean that algorithms can discover as well as illustrate patterns, the data on which they operate need to be understood in detail if spurious patterns are to avoided. In the following, we consider the case of automated analyses of electronic trace data left by students engaged in formal learning — so-called learning analytics — in an attempt to illustrate how the nature of data in a particular big data context, the intentions behind gathering and analyzing the data, and the appropriate analysis methods, all merit careful consideration."


## Business

-- where the hype (and bullshit comes form)
-- @marr2014big -- plenty of use cases from bussiness -- try to classify somehow? (not too much)

-- the question of maturity -- gartner's graph of technologies -- pozri diplomka (nejaká aktuálnejšia verzia?). -- search gartner hype cycle

## Society

-- some usual concerns
-- how to mitigate by lowering exhaustivity calling cognitive limits for the help

@florescu2012will -- posibility to extend official statistics with BD also kitchin2015opportunities

TODO -- use notes from file chap-philosophy of big data

## The improtance of visualization

-- vis for humans, machines do not need it (?) -- move to a different chapter
-- here just the societal responsibility of designers (first things first movement?, something for data visualists), and cartographers (harley, wood, crampton, -- radical cartography (Paglen?))

## more technical -- move somewhere else
* check later when you undestand it better :)
-- meta-data specifications for BD -- ask Radim & Tomas
-- ogc standard? -- refreshable wms? :P

NB https://www.theguardian.com/news/datablog/2008/jul/16/data-plural-singular

## risks

-- abuse at amazon -- @head2014worse
-- the world needs fixing, not disrupting: in <https://deardesignstudent.com/8-reasons-to-turn-down-that-startup-job-1f82a00ade34>
-- <https://medium.com/@freddiedeboer/the-three-hot-trends-in-silicon-valley-horseshit-95cc5a85e8a4>

## values

mayer2013big -- chapter value

examples: google flu trends:
ginsberg2009detecting
butler2013google -- critisism
cook2011assessing,
dugas2012google,

see also
@halevy2009unreasonable -- see video: https://www.youtube.com/watch?v=yvDCzhbjYWs

data creation in facilites:
-- cern obviously
-- https://www.lsst.org/ telescope in chile
-- companies providing acessing productivity in places like distribution centers @lewitt2016more
-- smart homes?

or more topic-based structure? from promise and peril
-----------------------------------------------------
-- How to Make Sense of Big Data?
     -- Data Correlation or Scientific Models?
     -- How Should Theories be Crafted in an Age of Big Data?
     -- Visualization as a Sense-Making Tool
     -- Bias-Free Interpretation of Big Data?
     -- Is More Actually Less?
     -- Correlations, Causality and Strategic Decision-making
-- How Should Big Data Abuses be Addressed?
     --Regulation, Contracts or Other Approaches?


from presentations at information+ 2016
----------

Catherine D’Ignazio 

Creative Data Literacy: Bridging the Gap Between the Data Haves and Have-nots

Communities are swimming in data—demographic data, participation data, government data, social media data—but very few understand what to do with it. Though governments and foundations are creating open data portals and corporations are creating APIs, these rarely focus on use, usability, building community or creating impact. So although there is an explosion of data, there is a significant lag in data literacy at the scale of communities and individuals. This creates a situation of data-haves and have-nots. But there are emerging technocultural practices that combine participation, creativity, and context to connect data to everyday life. These include citizen science, data journalism, novel public engagement in government processes, and participatory data art. This talk surveys these practices both lovingly and critically, including their premises, aspirations and the challenges to creating citizens that are truly empowered with data.

Yanni Loukissas

Local Data: Learning to Look at Big Data as Aggregates

Data are local: made by people and their dutiful machines, at a time, in a place, with the instruments at hand, using limited resources, to reach disciplined audiences. Although they are reliably transferred across digital communication networks, everywhere data remain marked by local artifacts: traces of the conditions and values that are particular to their origins. Even big data—a term that valorizes the scale and perceived autonomy of today’s data practices—are no more than aggregations of local knowledge, grounded in and inseparable from their social and material beginnings. This paper argues that we should learn to look at big data as data aggregates: collections of local data with varied and discordant infrastructural ties. In other words, if we are to fully understand the opportunities and pitfalls of big data, we must begin to see both the forest and the trees. 



